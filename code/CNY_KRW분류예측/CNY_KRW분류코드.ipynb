{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab7c9db",
   "metadata": {},
   "source": [
    "# 다시 손질"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b90957d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4282 entries, 0 to 4281\n",
      "Columns: 209 entries, Date to 글로벌EPU_PPP기준\n",
      "dtypes: float64(189), int64(18), object(2)\n",
      "memory usage: 6.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Date', '미환율_종가', '미환율_시가', '미환율_고가', '미환율_저가', '미환율_변동%', 'WTI유_종가',\n",
       "       'WTI유_시가', 'WTI유_고가', 'WTI유_저가',\n",
       "       ...\n",
       "       '일본엔_검색량', '유로(EUR)_검색량', 'S&P 500_검색량', '코스피_검색량', '일본EPU', '중국EPU',\n",
       "       '한국EPU', '미국EPU', '글로벌EPU_명목GDP기준', '글로벌EPU_PPP기준'],\n",
       "      dtype='object', length=209)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('어쩌면최종데이터.csv')\n",
    "df.info()\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0b523e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', '미환율_종가', '미환율_시가', '미환율_고가', '미환율_저가', '미환율_변동%', 'WTI유_종가', 'WTI유_시가', 'WTI유_고가', 'WTI유_저가', 'WTI유_거래량', 'WTI유_변동%', '금_종가', '금_시가', '금_고가', '금_저가', '금_거래량', '금_변동%', 'S&P500_종가', 'S&P500_시가', 'S&P500_고가', 'S&P500_저가', 'S&P500_변동%', '다우존스_종가', '다우존스_시가', '다우존스_고가', '다우존스_저가', '다우존스_거래량', '다우존스_변동%', '상해종합_종가', '상해종합_시가', '상해종합_고가', '상해종합_저가', '상해종합_거래량', '상해종합_변동%', '닛케이_종가', '닛케이_시가', '닛케이_고가', '닛케이_저가', '닛케이_변동%', '코스피_종가', '코스피_시가', '코스피_고가', '코스피_저가', '코스피_거래량', '코스피_변동%', '나스닥_종가', '나스닥_시가', '나스닥_고가', '나스닥_저가', '나스닥_거래량', '나스닥_변동%', 'VIX_종가', 'VIX_시가', 'VIX_고가', 'VIX_저가', 'VIX_거래량', 'VIX_변동%', '시가총액_전체', '시가총액_외국인보유', '시가총액_비율', '주식수_전체', '주식수_외국인보유', '주식수_비율', 'CD금리(91일)', '국고채(3년)', '한국정책금리', '미국정책금리', 'return', 'log_return', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'sma_5', 'std_5', 'sma_10', 'std_10', 'sma_20', 'std_20', 'sma_60', 'std_60', 'bb_upper', 'bb_lower', 'bb_width', 'volatility_20', 'momentum_5', 'momentum_10', 'momentum_20', 'rsi_14', 'macd', 'macd_sig', 'macd_hist', 'sto_k', 'sto_d', 'Volume', 'alpha2', 'alpha4', 'alpha7', 'alpha9', 'VWAP', 'alpha10', 'alpha11', 'alpha12', 'alpha13', 'alpha16', 'adv20', 'alpha17', 'alpha18', 'alpha19', 'alpha20', 'alpha21', 'alpha23', 'alpha24', 'alpha25', 'alpha27', 'alpha29', 'alpha30', 'alpha32', 'alpha33', 'alpha34', 'alpha35', 'alpha36', 'alpha37', 'returns', 'alpha38', 'alpha39', 'alpha41', 'alpha42', 'alpha43', 'alpha46', 'alpha47', 'alpha48', 'alpha49', 'alpha51', 'alpha52', 'alpha53', 'alpha54', 'cap', 'alpha56', 'alpha57', 'alpha58', 'alpha60', 'adv180', 'adv120', 'adv60', 'adv50', 'adv15', 'alpha61', 'alpha62', 'alpha63', 'alpha64', 'alpha65', 'alpha66', 'alpha67', 'alpha68', 'alpha69', 'alpha70', 'adv5', 'adv10', 'adv30', 'adv40', 'adv81', 'adv150', 'alpha72', 'alpha73', 'alpha74', 'alpha75', 'alpha76', 'alpha77', 'alpha79', 'alpha81', 'alpha83', 'alpha84', 'alpha85', 'alpha86', 'alpha87', 'alpha90', 'alpha92', 'alpha93', 'alpha95', 'alpha98', 'alpha99', 'alpha101', '기준년월', '한국(M1)조원', '한국(M1)변동%', '한국(M2)조원', '한국(M2)변동%', '미국(M1)십억달러', '미국(M2)십억달러', '소비자심리지수', '생산자물가지수', '산업생산지수', '외환보유액(억달러)', '경상수지', '미국소비자물가지수', '비트코인_검색량', '일본엔_검색량', '유로(EUR)_검색량', 'S&P 500_검색량', '코스피_검색량', '일본EPU', '중국EPU', '한국EPU', '미국EPU', '글로벌EPU_명목GDP기준', '글로벌EPU_PPP기준']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "50608507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제거된 컬럼 수: 122\n",
      "남은 컬럼 수: 87\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 불러오기\n",
    "df = df\n",
    "\n",
    "# ✅ 미국 환율 관련 가격 컬럼만 제거\n",
    "price_cols = [col for col in df.columns if col.startswith('미환율_') and any(x in col for x in ['시가', '종가', '고가', '저가', '변동%'])]\n",
    "\n",
    "# ✅ 파생변수 관련 컬럼 제거 조건\n",
    "technical_prefixes = [\n",
    "    'return', 'log_return', 'lag_', 'sma_', 'std_', 'bb_', 'volatility_',\n",
    "    'momentum_', 'rsi_', 'macd', 'sto_', 'VWAP', 'Volume',\n",
    "    'alpha', 'adv', 'cap'\n",
    "]\n",
    "technical_cols = [col for col in df.columns if any(col.startswith(prefix) for prefix in technical_prefixes)]\n",
    "\n",
    "# ✅ 제거 컬럼 최종 리스트\n",
    "drop_cols = list(set(price_cols + technical_cols))\n",
    "\n",
    "# ✅ 제거 실행\n",
    "df_cleaned = df.drop(columns=drop_cols)\n",
    "\n",
    "# ✅ 결과 확인\n",
    "print(f\"제거된 컬럼 수: {len(drop_cols)}\")\n",
    "print(f\"남은 컬럼 수: {len(df_cleaned.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "76cac453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'WTI유_종가', 'WTI유_시가', 'WTI유_고가', 'WTI유_저가', 'WTI유_거래량', 'WTI유_변동%', '금_종가', '금_시가', '금_고가', '금_저가', '금_거래량', '금_변동%', 'S&P500_종가', 'S&P500_시가', 'S&P500_고가', 'S&P500_저가', 'S&P500_변동%', '다우존스_종가', '다우존스_시가', '다우존스_고가', '다우존스_저가', '다우존스_거래량', '다우존스_변동%', '상해종합_종가', '상해종합_시가', '상해종합_고가', '상해종합_저가', '상해종합_거래량', '상해종합_변동%', '닛케이_종가', '닛케이_시가', '닛케이_고가', '닛케이_저가', '닛케이_변동%', '코스피_종가', '코스피_시가', '코스피_고가', '코스피_저가', '코스피_거래량', '코스피_변동%', '나스닥_종가', '나스닥_시가', '나스닥_고가', '나스닥_저가', '나스닥_거래량', '나스닥_변동%', 'VIX_종가', 'VIX_시가', 'VIX_고가', 'VIX_저가', 'VIX_거래량', 'VIX_변동%', '시가총액_전체', '시가총액_외국인보유', '시가총액_비율', '주식수_전체', '주식수_외국인보유', '주식수_비율', 'CD금리(91일)', '국고채(3년)', '한국정책금리', '미국정책금리', '기준년월', '한국(M1)조원', '한국(M1)변동%', '한국(M2)조원', '한국(M2)변동%', '미국(M1)십억달러', '미국(M2)십억달러', '소비자심리지수', '생산자물가지수', '산업생산지수', '외환보유액(억달러)', '경상수지', '미국소비자물가지수', '비트코인_검색량', '일본엔_검색량', '유로(EUR)_검색량', 'S&P 500_검색량', '코스피_검색량', '일본EPU', '중국EPU', '한국EPU', '미국EPU', '글로벌EPU_명목GDP기준', '글로벌EPU_PPP기준']\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2cd56278",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cny=pd.read_csv('파생변수포함위안화데이터.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2c5330a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cny.drop(columns=['Unnamed: 0'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78212465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Close', 'Open', 'High', 'Low', '변동%', 'return', 'log_return', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'sma_5', 'std_5', 'sma_10', 'std_10', 'sma_20', 'std_20', 'sma_60', 'std_60', 'bb_upper', 'bb_lower', 'bb_width', 'volatility_20', 'momentum_5', 'momentum_10', 'momentum_20', 'rsi_14', 'macd', 'macd_sig', 'macd_hist', 'sto_k', 'sto_d', 'M2(100 million yuan)', 'Volume', 'M0(100 million yuan)', 'alpha2', 'alpha4', 'alpha7', 'alpha9', 'VWAP', 'alpha10', 'alpha11', 'alpha12', 'alpha13', 'alpha16', 'adv20', 'alpha17', 'alpha18', 'alpha20', 'alpha21', 'alpha23', 'alpha24', 'alpha25', 'alpha27', 'alpha28', 'alpha29', 'alpha30', 'alpha31', 'alpha33', 'alpha34', 'alpha35', 'alpha37', 'returns', 'alpha38', 'alpha41', 'alpha42', 'alpha43', 'alpha46', 'alpha47', 'alpha49', 'cap', 'alpha56', 'alpha57', 'alpha58', 'alpha51', 'alpha54', 'alpha60', 'adv180', 'adv120', 'adv60', 'adv50', 'adv15', 'alpha61', 'alpha62', 'alpha64', 'alpha65', 'alpha67', 'alpha68', 'alpha70', 'adv5', 'adv10', 'adv30', 'adv40', 'adv81', 'adv150', 'alpha73', 'alpha74', 'alpha75', 'alpha76', 'alpha77', 'alpha79', 'alpha81', 'alpha83', 'alpha84', 'alpha86', 'alpha87', 'alpha90', 'alpha92', 'alpha93', 'alpha98', 'alpha99', 'alpha100', 'alpha101']\n"
     ]
    }
   ],
   "source": [
    "print(df_cny.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e6b61ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. 날짜 형식 통일\n",
    "df_cleaned['Date'] = pd.to_datetime(df_cleaned['Date'])\n",
    "df_cny['Date'] = pd.to_datetime(df_cny['Date'])\n",
    "\n",
    "# 2. 중국환율 가격 컬럼명 변경\n",
    "rename_map = {\n",
    "    'Open': '중국환율_시가',\n",
    "    'High': '중국환율_고가',\n",
    "    'Low': '중국환율_저가',\n",
    "    'Close': '중국환율_종가',\n",
    "    '변동%': '중국환율_변동%'\n",
    "}\n",
    "df_cny_renamed = df_cny.rename(columns=rename_map)\n",
    "\n",
    "# 3. 병합 대상 컬럼 구성\n",
    "cols_to_add = ['Date'] + list(rename_map.values()) + [\n",
    "    col for col in df_cny_renamed.columns if col not in ['Date'] + list(rename_map.keys())\n",
    "]\n",
    "\n",
    "# 4. 병합\n",
    "df_final = df_cleaned.merge(df_cny_renamed[cols_to_add], on='Date', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7afa80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'WTI유_종가', 'WTI유_시가', 'WTI유_고가', 'WTI유_저가', 'WTI유_거래량', 'WTI유_변동%', '금_종가', '금_시가', '금_고가', '금_저가', '금_거래량', '금_변동%', 'S&P500_종가', 'S&P500_시가', 'S&P500_고가', 'S&P500_저가', 'S&P500_변동%', '다우존스_종가', '다우존스_시가', '다우존스_고가', '다우존스_저가', '다우존스_거래량', '다우존스_변동%', '상해종합_종가', '상해종합_시가', '상해종합_고가', '상해종합_저가', '상해종합_거래량', '상해종합_변동%', '닛케이_종가', '닛케이_시가', '닛케이_고가', '닛케이_저가', '닛케이_변동%', '코스피_종가', '코스피_시가', '코스피_고가', '코스피_저가', '코스피_거래량', '코스피_변동%', '나스닥_종가', '나스닥_시가', '나스닥_고가', '나스닥_저가', '나스닥_거래량', '나스닥_변동%', 'VIX_종가', 'VIX_시가', 'VIX_고가', 'VIX_저가', 'VIX_거래량', 'VIX_변동%', '시가총액_전체', '시가총액_외국인보유', '시가총액_비율', '주식수_전체', '주식수_외국인보유', '주식수_비율', 'CD금리(91일)', '국고채(3년)', '한국정책금리', '미국정책금리', '기준년월', '한국(M1)조원', '한국(M1)변동%', '한국(M2)조원', '한국(M2)변동%', '미국(M1)십억달러', '미국(M2)십억달러', '소비자심리지수', '생산자물가지수', '산업생산지수', '외환보유액(억달러)', '경상수지', '미국소비자물가지수', '비트코인_검색량', '일본엔_검색량', '유로(EUR)_검색량', 'S&P 500_검색량', '코스피_검색량', '일본EPU', '중국EPU', '한국EPU', '미국EPU', '글로벌EPU_명목GDP기준', '글로벌EPU_PPP기준', '중국환율_시가', '중국환율_고가', '중국환율_저가', '중국환율_종가', '중국환율_변동%', '중국환율_종가', '중국환율_시가', '중국환율_고가', '중국환율_저가', '중국환율_변동%', 'return', 'log_return', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'sma_5', 'std_5', 'sma_10', 'std_10', 'sma_20', 'std_20', 'sma_60', 'std_60', 'bb_upper', 'bb_lower', 'bb_width', 'volatility_20', 'momentum_5', 'momentum_10', 'momentum_20', 'rsi_14', 'macd', 'macd_sig', 'macd_hist', 'sto_k', 'sto_d', 'M2(100 million yuan)', 'Volume', 'M0(100 million yuan)', 'alpha2', 'alpha4', 'alpha7', 'alpha9', 'VWAP', 'alpha10', 'alpha11', 'alpha12', 'alpha13', 'alpha16', 'adv20', 'alpha17', 'alpha18', 'alpha20', 'alpha21', 'alpha23', 'alpha24', 'alpha25', 'alpha27', 'alpha28', 'alpha29', 'alpha30', 'alpha31', 'alpha33', 'alpha34', 'alpha35', 'alpha37', 'returns', 'alpha38', 'alpha41', 'alpha42', 'alpha43', 'alpha46', 'alpha47', 'alpha49', 'cap', 'alpha56', 'alpha57', 'alpha58', 'alpha51', 'alpha54', 'alpha60', 'adv180', 'adv120', 'adv60', 'adv50', 'adv15', 'alpha61', 'alpha62', 'alpha64', 'alpha65', 'alpha67', 'alpha68', 'alpha70', 'adv5', 'adv10', 'adv30', 'adv40', 'adv81', 'adv150', 'alpha73', 'alpha74', 'alpha75', 'alpha76', 'alpha77', 'alpha79', 'alpha81', 'alpha83', 'alpha84', 'alpha86', 'alpha87', 'alpha90', 'alpha92', 'alpha93', 'alpha98', 'alpha99', 'alpha100', 'alpha101']\n"
     ]
    }
   ],
   "source": [
    "print(df_final.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e81438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "452da780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9206cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('위안화최종데이터.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7f9d7-cf6a-4a5d-926f-a640ffbf4e6e",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d7a0097-57cf-41a1-8eed-d36392a235b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상승 / 하락및보합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c4659fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if isinstance(df[col], pd.Series) and df[col].apply(lambda x: isinstance(x, pd.Series)).any():\n",
    "        df[col] = df[col].apply(lambda x: x.squeeze() if isinstance(x, pd.Series) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ecaa6bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['중국환율_종가', '중국환율_시가', '중국환율_고가', '중국환율_저가', '중국환율_변동%']:\n",
    "    df[col] = df[col].squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea001c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :흰색_확인_표시: 결측값 개수 및 비율 계산\n",
    "null_df = pd.DataFrame({\n",
    "    '결측값 개수': df.isnull().sum(),\n",
    "    '결측 비율 (%)': df.isnull().mean() * 100\n",
    "})\n",
    "# :흰색_확인_표시: 'alpha'가 컬럼명에 포함된 것 중에서만 결측 비율 5% 이상인 컬럼 필터링\n",
    "alpha_cols = [col for col in df.columns if 'alpha' in col]\n",
    "alpha_null_df = null_df.loc[alpha_cols]\n",
    "drop_cols = alpha_null_df[alpha_null_df['결측 비율 (%)'] >= 5].index.tolist()\n",
    "# :흰색_확인_표시: 해당 alpha 컬럼들만 삭제\n",
    "df = df.drop(columns=drop_cols)\n",
    "# :흰색_확인_표시: 남은 결측값 전체 삭제 (단, alpha 외 컬럼 포함)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f464e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>중국환율_시가</th>\n",
       "      <th>중국환율_고가</th>\n",
       "      <th>중국환율_저가</th>\n",
       "      <th>중국환율_종가</th>\n",
       "      <th>중국환율_변동%</th>\n",
       "      <th>WTI유_종가</th>\n",
       "      <th>WTI유_시가</th>\n",
       "      <th>WTI유_고가</th>\n",
       "      <th>WTI유_저가</th>\n",
       "      <th>...</th>\n",
       "      <th>alpha86</th>\n",
       "      <th>alpha87</th>\n",
       "      <th>alpha90</th>\n",
       "      <th>alpha92</th>\n",
       "      <th>alpha93</th>\n",
       "      <th>alpha98</th>\n",
       "      <th>alpha99</th>\n",
       "      <th>alpha100</th>\n",
       "      <th>alpha101</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>197.01</td>\n",
       "      <td>197.27</td>\n",
       "      <td>195.32</td>\n",
       "      <td>195.93</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>58.21</td>\n",
       "      <td>60.16</td>\n",
       "      <td>60.43</td>\n",
       "      <td>57.91</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-429.0</td>\n",
       "      <td>-847.000000</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-2795.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.533027</td>\n",
       "      <td>-0.553562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>197.37</td>\n",
       "      <td>198.19</td>\n",
       "      <td>196.68</td>\n",
       "      <td>196.97</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>62.05</td>\n",
       "      <td>63.49</td>\n",
       "      <td>63.92</td>\n",
       "      <td>61.48</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2835.0</td>\n",
       "      <td>-750.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>-2467.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.880029</td>\n",
       "      <td>-0.264725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>196.34</td>\n",
       "      <td>198.16</td>\n",
       "      <td>196.23</td>\n",
       "      <td>197.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.86</td>\n",
       "      <td>63.41</td>\n",
       "      <td>61.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3963.0</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>-1461.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.105074</td>\n",
       "      <td>0.533402</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>195.89</td>\n",
       "      <td>197.32</td>\n",
       "      <td>195.65</td>\n",
       "      <td>196.35</td>\n",
       "      <td>0.23</td>\n",
       "      <td>62.79</td>\n",
       "      <td>62.34</td>\n",
       "      <td>63.31</td>\n",
       "      <td>61.99</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3735.0</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>-1055.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052260</td>\n",
       "      <td>0.275284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-23</td>\n",
       "      <td>195.59</td>\n",
       "      <td>196.05</td>\n",
       "      <td>194.70</td>\n",
       "      <td>195.89</td>\n",
       "      <td>0.15</td>\n",
       "      <td>62.27</td>\n",
       "      <td>64.00</td>\n",
       "      <td>64.87</td>\n",
       "      <td>61.53</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3410.0</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>-486.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.440872</td>\n",
       "      <td>0.222058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>2009-10-16</td>\n",
       "      <td>169.48</td>\n",
       "      <td>170.99</td>\n",
       "      <td>169.02</td>\n",
       "      <td>169.21</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>78.53</td>\n",
       "      <td>77.76</td>\n",
       "      <td>78.75</td>\n",
       "      <td>76.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1543.0</td>\n",
       "      <td>-92.833333</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>-1839.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.791072</td>\n",
       "      <td>-0.136986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>2009-10-15</td>\n",
       "      <td>169.72</td>\n",
       "      <td>170.38</td>\n",
       "      <td>168.48</td>\n",
       "      <td>169.61</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>77.58</td>\n",
       "      <td>75.34</td>\n",
       "      <td>77.97</td>\n",
       "      <td>74.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-332.0</td>\n",
       "      <td>-144.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>-1665.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.120217</td>\n",
       "      <td>-0.057864</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>2009-10-14</td>\n",
       "      <td>170.94</td>\n",
       "      <td>171.81</td>\n",
       "      <td>169.35</td>\n",
       "      <td>169.71</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>75.18</td>\n",
       "      <td>74.40</td>\n",
       "      <td>75.53</td>\n",
       "      <td>74.40</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-333.0</td>\n",
       "      <td>-318.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>-1176.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.952238</td>\n",
       "      <td>-0.499797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>2009-10-13</td>\n",
       "      <td>170.95</td>\n",
       "      <td>171.68</td>\n",
       "      <td>170.71</td>\n",
       "      <td>171.68</td>\n",
       "      <td>0.30</td>\n",
       "      <td>74.15</td>\n",
       "      <td>73.17</td>\n",
       "      <td>74.55</td>\n",
       "      <td>72.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3301.0</td>\n",
       "      <td>-3636.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>-775.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.579261</td>\n",
       "      <td>0.751802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>2009-10-09</td>\n",
       "      <td>170.42</td>\n",
       "      <td>171.28</td>\n",
       "      <td>170.13</td>\n",
       "      <td>170.66</td>\n",
       "      <td>0.05</td>\n",
       "      <td>71.77</td>\n",
       "      <td>71.42</td>\n",
       "      <td>72.35</td>\n",
       "      <td>70.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1368.0</td>\n",
       "      <td>-724.000000</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>-54.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.525106</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2760 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  중국환율_시가  중국환율_고가  중국환율_저가  중국환율_종가  중국환율_변동%  WTI유_종가  \\\n",
       "0     2025-04-30   197.01   197.27   195.32   195.93     -0.55    58.21   \n",
       "1     2025-04-28   197.37   198.19   196.68   196.97     -0.20    62.05   \n",
       "2     2025-04-25   196.34   198.16   196.23   197.37      0.52    63.02   \n",
       "3     2025-04-24   195.89   197.32   195.65   196.35      0.23    62.79   \n",
       "4     2025-04-23   195.59   196.05   194.70   195.89      0.15    62.27   \n",
       "...          ...      ...      ...      ...      ...       ...      ...   \n",
       "2755  2009-10-16   169.48   170.99   169.02   169.21     -0.24    78.53   \n",
       "2756  2009-10-15   169.72   170.38   168.48   169.61     -0.06    77.58   \n",
       "2757  2009-10-14   170.94   171.81   169.35   169.71     -1.15    75.18   \n",
       "2758  2009-10-13   170.95   171.68   170.71   171.68      0.30    74.15   \n",
       "2759  2009-10-09   170.42   171.28   170.13   170.66      0.05    71.77   \n",
       "\n",
       "      WTI유_시가  WTI유_고가  WTI유_저가  ...  alpha86  alpha87      alpha90   alpha92  \\\n",
       "0       60.16    60.43    57.91  ...       -1   -429.0  -847.000000  0.305556   \n",
       "1       63.49    63.92    61.48  ...       -1  -2835.0  -750.166667  0.055556   \n",
       "2       62.86    63.41    61.80  ...       -1  -3963.0 -1212.166667  0.055556   \n",
       "3       62.34    63.31    61.99  ...       -1  -3735.0 -1212.166667  0.111111   \n",
       "4       64.00    64.87    61.53  ...       -1  -3410.0 -1212.166667  0.166667   \n",
       "...       ...      ...      ...  ...      ...      ...          ...       ...   \n",
       "2755    77.76    78.75    76.82  ...       -1  -1543.0   -92.833333  0.055556   \n",
       "2756    75.34    77.97    74.79  ...       -1   -332.0  -144.166667  0.083333   \n",
       "2757    74.40    75.53    74.40  ...       -1   -333.0  -318.000000  0.166667   \n",
       "2758    73.17    74.55    72.83  ...       -1  -3301.0 -3636.500000  0.250000   \n",
       "2759    71.42    72.35    70.62  ...       -1  -1368.0  -724.000000  0.638889   \n",
       "\n",
       "       alpha93  alpha98  alpha99  alpha100  alpha101  target  \n",
       "0     0.000059  -2795.5        0  1.533027 -0.553562       0  \n",
       "1     0.000268  -2467.5        0  1.880029 -0.264725       0  \n",
       "2     0.000482  -1461.0        0 -2.105074  0.533402       1  \n",
       "3     0.000994  -1055.0        0  0.052260  0.275284       0  \n",
       "4     0.002033   -486.0        0 -3.440872  0.222058       0  \n",
       "...        ...      ...      ...       ...       ...     ...  \n",
       "2755  0.002801  -1839.5       -1 -0.791072 -0.136986       0  \n",
       "2756  0.002950  -1665.5        0 -2.120217 -0.057864       0  \n",
       "2757  0.001805  -1176.5        0 -0.952238 -0.499797       0  \n",
       "2758  0.001289   -775.0        0 -2.579261  0.751802       1  \n",
       "2759  0.000613    -54.5       -1 -1.525106  0.208514       0  \n",
       "\n",
       "[2760 rows x 201 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f733493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('위안화최종데이터.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08c3c83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " '중국환율_시가',\n",
       " '중국환율_고가',\n",
       " '중국환율_저가',\n",
       " '중국환율_종가',\n",
       " '중국환율_변동%',\n",
       " 'WTI유_종가',\n",
       " 'WTI유_시가',\n",
       " 'WTI유_고가',\n",
       " 'WTI유_저가',\n",
       " 'WTI유_거래량',\n",
       " 'WTI유_변동%',\n",
       " '금_종가',\n",
       " '금_시가',\n",
       " '금_고가',\n",
       " '금_저가',\n",
       " '금_거래량',\n",
       " '금_변동%',\n",
       " 'S&P500_종가',\n",
       " 'S&P500_시가',\n",
       " 'S&P500_고가',\n",
       " 'S&P500_저가',\n",
       " 'S&P500_변동%',\n",
       " '다우존스_종가',\n",
       " '다우존스_시가',\n",
       " '다우존스_고가',\n",
       " '다우존스_저가',\n",
       " '다우존스_거래량',\n",
       " '다우존스_변동%',\n",
       " '상해종합_종가',\n",
       " '상해종합_시가',\n",
       " '상해종합_고가',\n",
       " '상해종합_저가',\n",
       " '상해종합_거래량',\n",
       " '상해종합_변동%',\n",
       " '닛케이_종가',\n",
       " '닛케이_시가',\n",
       " '닛케이_고가',\n",
       " '닛케이_저가',\n",
       " '닛케이_변동%',\n",
       " '코스피_종가',\n",
       " '코스피_시가',\n",
       " '코스피_고가',\n",
       " '코스피_저가',\n",
       " '코스피_거래량',\n",
       " '코스피_변동%',\n",
       " '나스닥_종가',\n",
       " '나스닥_시가',\n",
       " '나스닥_고가',\n",
       " '나스닥_저가',\n",
       " '나스닥_거래량',\n",
       " '나스닥_변동%',\n",
       " 'VIX_종가',\n",
       " 'VIX_시가',\n",
       " 'VIX_고가',\n",
       " 'VIX_저가',\n",
       " 'VIX_거래량',\n",
       " 'VIX_변동%',\n",
       " '시가총액_전체',\n",
       " '시가총액_외국인보유',\n",
       " '시가총액_비율',\n",
       " '주식수_전체',\n",
       " '주식수_외국인보유',\n",
       " '주식수_비율',\n",
       " 'CD금리(91일)',\n",
       " '국고채(3년)',\n",
       " '한국정책금리',\n",
       " '미국정책금리',\n",
       " '기준년월',\n",
       " '한국(M1)조원',\n",
       " '한국(M1)변동%',\n",
       " '한국(M2)조원',\n",
       " '한국(M2)변동%',\n",
       " '미국(M1)십억달러',\n",
       " '미국(M2)십억달러',\n",
       " '소비자심리지수',\n",
       " '생산자물가지수',\n",
       " '산업생산지수',\n",
       " '외환보유액(억달러)',\n",
       " '경상수지',\n",
       " '미국소비자물가지수',\n",
       " '비트코인_검색량',\n",
       " '일본엔_검색량',\n",
       " '유로(EUR)_검색량',\n",
       " 'S&P 500_검색량',\n",
       " '코스피_검색량',\n",
       " '일본EPU',\n",
       " '중국EPU',\n",
       " '한국EPU',\n",
       " '미국EPU',\n",
       " '글로벌EPU_명목GDP기준',\n",
       " '글로벌EPU_PPP기준',\n",
       " 'return',\n",
       " 'log_return',\n",
       " 'lag_1',\n",
       " 'lag_2',\n",
       " 'lag_3',\n",
       " 'lag_4',\n",
       " 'lag_5',\n",
       " 'sma_5',\n",
       " 'std_5',\n",
       " 'sma_10',\n",
       " 'std_10',\n",
       " 'sma_20',\n",
       " 'std_20',\n",
       " 'sma_60',\n",
       " 'std_60',\n",
       " 'bb_upper',\n",
       " 'bb_lower',\n",
       " 'bb_width',\n",
       " 'volatility_20',\n",
       " 'momentum_5',\n",
       " 'momentum_10',\n",
       " 'momentum_20',\n",
       " 'rsi_14',\n",
       " 'macd',\n",
       " 'macd_sig',\n",
       " 'macd_hist',\n",
       " 'sto_k',\n",
       " 'sto_d',\n",
       " 'M2(100 million yuan)',\n",
       " 'Volume',\n",
       " 'M0(100 million yuan)',\n",
       " 'alpha2',\n",
       " 'alpha4',\n",
       " 'alpha7',\n",
       " 'alpha9',\n",
       " 'VWAP',\n",
       " 'alpha10',\n",
       " 'alpha11',\n",
       " 'alpha12',\n",
       " 'alpha13',\n",
       " 'alpha16',\n",
       " 'adv20',\n",
       " 'alpha17',\n",
       " 'alpha18',\n",
       " 'alpha20',\n",
       " 'alpha21',\n",
       " 'alpha23',\n",
       " 'alpha24',\n",
       " 'alpha25',\n",
       " 'alpha27',\n",
       " 'alpha28',\n",
       " 'alpha29',\n",
       " 'alpha30',\n",
       " 'alpha31',\n",
       " 'alpha33',\n",
       " 'alpha34',\n",
       " 'alpha35',\n",
       " 'alpha37',\n",
       " 'returns',\n",
       " 'alpha38',\n",
       " 'alpha41',\n",
       " 'alpha42',\n",
       " 'alpha43',\n",
       " 'alpha46',\n",
       " 'alpha47',\n",
       " 'alpha49',\n",
       " 'cap',\n",
       " 'alpha56',\n",
       " 'alpha57',\n",
       " 'alpha58',\n",
       " 'alpha51',\n",
       " 'alpha54',\n",
       " 'alpha60',\n",
       " 'adv180',\n",
       " 'adv120',\n",
       " 'adv60',\n",
       " 'adv50',\n",
       " 'adv15',\n",
       " 'alpha61',\n",
       " 'alpha62',\n",
       " 'alpha64',\n",
       " 'alpha65',\n",
       " 'alpha67',\n",
       " 'alpha68',\n",
       " 'alpha70',\n",
       " 'adv5',\n",
       " 'adv10',\n",
       " 'adv30',\n",
       " 'adv40',\n",
       " 'adv81',\n",
       " 'adv150',\n",
       " 'alpha73',\n",
       " 'alpha74',\n",
       " 'alpha75',\n",
       " 'alpha76',\n",
       " 'alpha77',\n",
       " 'alpha79',\n",
       " 'alpha81',\n",
       " 'alpha83',\n",
       " 'alpha84',\n",
       " 'alpha86',\n",
       " 'alpha87',\n",
       " 'alpha90',\n",
       " 'alpha92',\n",
       " 'alpha93',\n",
       " 'alpha98',\n",
       " 'alpha99',\n",
       " 'alpha100',\n",
       " 'alpha101',\n",
       " 'next_day_close',\n",
       " 'change',\n",
       " 'target']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28c6e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3187cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9df2587",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('위안화최종데이터.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eca87f1-c14f-4948-bd5e-e71c2c697b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0.7, alpha: 0.49, threshold: 0.4900, class_weight_1: 11.0, macro f1-score: 0.7630\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92       447\n",
      "           1       0.69      0.53      0.60       101\n",
      "\n",
      "    accuracy                           0.87       548\n",
      "   macro avg       0.80      0.74      0.76       548\n",
      "weighted avg       0.86      0.87      0.86       548\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[423  24]\n",
      " [ 47  54]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0.7\n",
    "alpha_list = [0.49, 0.5]       # 성능 중심 구간\n",
    "threshold_list = [0.49, 0.5]   # 유효 범위 압축\n",
    "weight_list = [10, 11]     # class_weight_1 \n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = LSTM(64, return_sequences=True)(inputs)\n",
    "x = LSTM(32)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9829294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('위안화최종데이터.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "841f4e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>중국환율_시가</th>\n",
       "      <th>중국환율_고가</th>\n",
       "      <th>중국환율_저가</th>\n",
       "      <th>중국환율_종가</th>\n",
       "      <th>중국환율_변동%</th>\n",
       "      <th>WTI유_종가</th>\n",
       "      <th>WTI유_시가</th>\n",
       "      <th>WTI유_고가</th>\n",
       "      <th>WTI유_저가</th>\n",
       "      <th>...</th>\n",
       "      <th>alpha90</th>\n",
       "      <th>alpha92</th>\n",
       "      <th>alpha93</th>\n",
       "      <th>alpha98</th>\n",
       "      <th>alpha99</th>\n",
       "      <th>alpha100</th>\n",
       "      <th>alpha101</th>\n",
       "      <th>next_day_close</th>\n",
       "      <th>change</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>197.01</td>\n",
       "      <td>197.27</td>\n",
       "      <td>195.32</td>\n",
       "      <td>195.93</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>58.21</td>\n",
       "      <td>60.16</td>\n",
       "      <td>60.43</td>\n",
       "      <td>57.91</td>\n",
       "      <td>...</td>\n",
       "      <td>-847.000000</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-2795.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.533027</td>\n",
       "      <td>-0.553562</td>\n",
       "      <td>196.97</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>197.37</td>\n",
       "      <td>198.19</td>\n",
       "      <td>196.68</td>\n",
       "      <td>196.97</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>62.05</td>\n",
       "      <td>63.49</td>\n",
       "      <td>63.92</td>\n",
       "      <td>61.48</td>\n",
       "      <td>...</td>\n",
       "      <td>-750.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>-2467.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.880029</td>\n",
       "      <td>-0.264725</td>\n",
       "      <td>197.37</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>196.34</td>\n",
       "      <td>198.16</td>\n",
       "      <td>196.23</td>\n",
       "      <td>197.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.86</td>\n",
       "      <td>63.41</td>\n",
       "      <td>61.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>-1461.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.105074</td>\n",
       "      <td>0.533402</td>\n",
       "      <td>196.35</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>195.89</td>\n",
       "      <td>197.32</td>\n",
       "      <td>195.65</td>\n",
       "      <td>196.35</td>\n",
       "      <td>0.23</td>\n",
       "      <td>62.79</td>\n",
       "      <td>62.34</td>\n",
       "      <td>63.31</td>\n",
       "      <td>61.99</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>-1055.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052260</td>\n",
       "      <td>0.275284</td>\n",
       "      <td>195.89</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-23</td>\n",
       "      <td>195.59</td>\n",
       "      <td>196.05</td>\n",
       "      <td>194.70</td>\n",
       "      <td>195.89</td>\n",
       "      <td>0.15</td>\n",
       "      <td>62.27</td>\n",
       "      <td>64.00</td>\n",
       "      <td>64.87</td>\n",
       "      <td>61.53</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>-486.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.440872</td>\n",
       "      <td>0.222058</td>\n",
       "      <td>195.59</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>2009-10-21</td>\n",
       "      <td>169.94</td>\n",
       "      <td>173.46</td>\n",
       "      <td>169.94</td>\n",
       "      <td>172.45</td>\n",
       "      <td>1.07</td>\n",
       "      <td>81.37</td>\n",
       "      <td>78.69</td>\n",
       "      <td>82.00</td>\n",
       "      <td>77.64</td>\n",
       "      <td>...</td>\n",
       "      <td>-3636.500000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>1785.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.885031</td>\n",
       "      <td>0.712866</td>\n",
       "      <td>170.62</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>2009-10-20</td>\n",
       "      <td>171.71</td>\n",
       "      <td>173.88</td>\n",
       "      <td>170.62</td>\n",
       "      <td>170.62</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>79.09</td>\n",
       "      <td>79.61</td>\n",
       "      <td>80.05</td>\n",
       "      <td>78.05</td>\n",
       "      <td>...</td>\n",
       "      <td>-2079.500000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>766.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.111166</td>\n",
       "      <td>-0.334253</td>\n",
       "      <td>171.11</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>2009-10-19</td>\n",
       "      <td>170.31</td>\n",
       "      <td>172.89</td>\n",
       "      <td>170.31</td>\n",
       "      <td>171.11</td>\n",
       "      <td>1.12</td>\n",
       "      <td>79.61</td>\n",
       "      <td>78.56</td>\n",
       "      <td>79.69</td>\n",
       "      <td>78.05</td>\n",
       "      <td>...</td>\n",
       "      <td>-3636.500000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>-732.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.128779</td>\n",
       "      <td>0.309957</td>\n",
       "      <td>169.21</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>2009-10-16</td>\n",
       "      <td>169.48</td>\n",
       "      <td>170.99</td>\n",
       "      <td>169.02</td>\n",
       "      <td>169.21</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>78.53</td>\n",
       "      <td>77.76</td>\n",
       "      <td>78.75</td>\n",
       "      <td>76.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-92.833333</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>-1839.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.791072</td>\n",
       "      <td>-0.136986</td>\n",
       "      <td>169.61</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>2009-10-15</td>\n",
       "      <td>169.72</td>\n",
       "      <td>170.38</td>\n",
       "      <td>168.48</td>\n",
       "      <td>169.61</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>77.58</td>\n",
       "      <td>75.34</td>\n",
       "      <td>77.97</td>\n",
       "      <td>74.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-144.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>-1665.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.120217</td>\n",
       "      <td>-0.057864</td>\n",
       "      <td>169.71</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2757 rows × 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  중국환율_시가  중국환율_고가  중국환율_저가  중국환율_종가  중국환율_변동%  WTI유_종가  \\\n",
       "0    2025-04-30   197.01   197.27   195.32   195.93     -0.55    58.21   \n",
       "1    2025-04-28   197.37   198.19   196.68   196.97     -0.20    62.05   \n",
       "2    2025-04-25   196.34   198.16   196.23   197.37      0.52    63.02   \n",
       "3    2025-04-24   195.89   197.32   195.65   196.35      0.23    62.79   \n",
       "4    2025-04-23   195.59   196.05   194.70   195.89      0.15    62.27   \n",
       "...         ...      ...      ...      ...      ...       ...      ...   \n",
       "2752 2009-10-21   169.94   173.46   169.94   172.45      1.07    81.37   \n",
       "2753 2009-10-20   171.71   173.88   170.62   170.62     -0.29    79.09   \n",
       "2754 2009-10-19   170.31   172.89   170.31   171.11      1.12    79.61   \n",
       "2755 2009-10-16   169.48   170.99   169.02   169.21     -0.24    78.53   \n",
       "2756 2009-10-15   169.72   170.38   168.48   169.61     -0.06    77.58   \n",
       "\n",
       "      WTI유_시가  WTI유_고가  WTI유_저가  ...      alpha90   alpha92   alpha93  \\\n",
       "0       60.16    60.43    57.91  ...  -847.000000  0.305556  0.000059   \n",
       "1       63.49    63.92    61.48  ...  -750.166667  0.055556  0.000268   \n",
       "2       62.86    63.41    61.80  ... -1212.166667  0.055556  0.000482   \n",
       "3       62.34    63.31    61.99  ... -1212.166667  0.111111  0.000994   \n",
       "4       64.00    64.87    61.53  ... -1212.166667  0.166667  0.002033   \n",
       "...       ...      ...      ...  ...          ...       ...       ...   \n",
       "2752    78.69    82.00    77.64  ... -3636.500000  0.055556  0.000401   \n",
       "2753    79.61    80.05    78.05  ... -2079.500000  0.055556  0.000553   \n",
       "2754    78.56    79.69    78.05  ... -3636.500000  0.055556  0.001147   \n",
       "2755    77.76    78.75    76.82  ...   -92.833333  0.055556  0.002801   \n",
       "2756    75.34    77.97    74.79  ...  -144.166667  0.083333  0.002950   \n",
       "\n",
       "      alpha98  alpha99  alpha100  alpha101  next_day_close  change  target  \n",
       "0     -2795.5        0  1.533027 -0.553562          196.97    1.04       1  \n",
       "1     -2467.5        0  1.880029 -0.264725          197.37    0.40       0  \n",
       "2     -1461.0        0 -2.105074  0.533402          196.35   -1.02       0  \n",
       "3     -1055.0        0  0.052260  0.275284          195.89   -0.46       0  \n",
       "4      -486.0        0 -3.440872  0.222058          195.59   -0.30       0  \n",
       "...       ...      ...       ...       ...             ...     ...     ...  \n",
       "2752   1785.5        0 -1.885031  0.712866          170.62   -1.83       0  \n",
       "2753    766.0        0 -0.111166 -0.334253          171.11    0.49       0  \n",
       "2754   -732.0       -1 -1.128779  0.309957          169.21   -1.90       0  \n",
       "2755  -1839.5       -1 -0.791072 -0.136986          169.61    0.40       0  \n",
       "2756  -1665.5        0 -2.120217 -0.057864          169.71    0.10       0  \n",
       "\n",
       "[2757 rows x 203 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2733ca63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " '중국환율_시가',\n",
       " '중국환율_고가',\n",
       " '중국환율_저가',\n",
       " '중국환율_종가',\n",
       " '중국환율_변동%',\n",
       " 'WTI유_종가',\n",
       " 'WTI유_시가',\n",
       " 'WTI유_고가',\n",
       " 'WTI유_저가',\n",
       " 'WTI유_거래량',\n",
       " 'WTI유_변동%',\n",
       " '금_종가',\n",
       " '금_시가',\n",
       " '금_고가',\n",
       " '금_저가',\n",
       " '금_거래량',\n",
       " '금_변동%',\n",
       " 'S&P500_종가',\n",
       " 'S&P500_시가',\n",
       " 'S&P500_고가',\n",
       " 'S&P500_저가',\n",
       " 'S&P500_변동%',\n",
       " '다우존스_종가',\n",
       " '다우존스_시가',\n",
       " '다우존스_고가',\n",
       " '다우존스_저가',\n",
       " '다우존스_거래량',\n",
       " '다우존스_변동%',\n",
       " '상해종합_종가',\n",
       " '상해종합_시가',\n",
       " '상해종합_고가',\n",
       " '상해종합_저가',\n",
       " '상해종합_거래량',\n",
       " '상해종합_변동%',\n",
       " '닛케이_종가',\n",
       " '닛케이_시가',\n",
       " '닛케이_고가',\n",
       " '닛케이_저가',\n",
       " '닛케이_변동%',\n",
       " '코스피_종가',\n",
       " '코스피_시가',\n",
       " '코스피_고가',\n",
       " '코스피_저가',\n",
       " '코스피_거래량',\n",
       " '코스피_변동%',\n",
       " '나스닥_종가',\n",
       " '나스닥_시가',\n",
       " '나스닥_고가',\n",
       " '나스닥_저가',\n",
       " '나스닥_거래량',\n",
       " '나스닥_변동%',\n",
       " 'VIX_종가',\n",
       " 'VIX_시가',\n",
       " 'VIX_고가',\n",
       " 'VIX_저가',\n",
       " 'VIX_거래량',\n",
       " 'VIX_변동%',\n",
       " '시가총액_전체',\n",
       " '시가총액_외국인보유',\n",
       " '시가총액_비율',\n",
       " '주식수_전체',\n",
       " '주식수_외국인보유',\n",
       " '주식수_비율',\n",
       " 'CD금리(91일)',\n",
       " '국고채(3년)',\n",
       " '한국정책금리',\n",
       " '미국정책금리',\n",
       " '기준년월',\n",
       " '한국(M1)조원',\n",
       " '한국(M1)변동%',\n",
       " '한국(M2)조원',\n",
       " '한국(M2)변동%',\n",
       " '미국(M1)십억달러',\n",
       " '미국(M2)십억달러',\n",
       " '소비자심리지수',\n",
       " '생산자물가지수',\n",
       " '산업생산지수',\n",
       " '외환보유액(억달러)',\n",
       " '경상수지',\n",
       " '미국소비자물가지수',\n",
       " '비트코인_검색량',\n",
       " '일본엔_검색량',\n",
       " '유로(EUR)_검색량',\n",
       " 'S&P 500_검색량',\n",
       " '코스피_검색량',\n",
       " '일본EPU',\n",
       " '중국EPU',\n",
       " '한국EPU',\n",
       " '미국EPU',\n",
       " '글로벌EPU_명목GDP기준',\n",
       " '글로벌EPU_PPP기준',\n",
       " 'return',\n",
       " 'log_return',\n",
       " 'lag_1',\n",
       " 'lag_2',\n",
       " 'lag_3',\n",
       " 'lag_4',\n",
       " 'lag_5',\n",
       " 'sma_5',\n",
       " 'std_5',\n",
       " 'sma_10',\n",
       " 'std_10',\n",
       " 'sma_20',\n",
       " 'std_20',\n",
       " 'sma_60',\n",
       " 'std_60',\n",
       " 'bb_upper',\n",
       " 'bb_lower',\n",
       " 'bb_width',\n",
       " 'volatility_20',\n",
       " 'momentum_5',\n",
       " 'momentum_10',\n",
       " 'momentum_20',\n",
       " 'rsi_14',\n",
       " 'macd',\n",
       " 'macd_sig',\n",
       " 'macd_hist',\n",
       " 'sto_k',\n",
       " 'sto_d',\n",
       " 'M2(100 million yuan)',\n",
       " 'Volume',\n",
       " 'M0(100 million yuan)',\n",
       " 'alpha2',\n",
       " 'alpha4',\n",
       " 'alpha7',\n",
       " 'alpha9',\n",
       " 'VWAP',\n",
       " 'alpha10',\n",
       " 'alpha11',\n",
       " 'alpha12',\n",
       " 'alpha13',\n",
       " 'alpha16',\n",
       " 'adv20',\n",
       " 'alpha17',\n",
       " 'alpha18',\n",
       " 'alpha20',\n",
       " 'alpha21',\n",
       " 'alpha23',\n",
       " 'alpha24',\n",
       " 'alpha25',\n",
       " 'alpha27',\n",
       " 'alpha28',\n",
       " 'alpha29',\n",
       " 'alpha30',\n",
       " 'alpha31',\n",
       " 'alpha33',\n",
       " 'alpha34',\n",
       " 'alpha35',\n",
       " 'alpha37',\n",
       " 'returns',\n",
       " 'alpha38',\n",
       " 'alpha41',\n",
       " 'alpha42',\n",
       " 'alpha43',\n",
       " 'alpha46',\n",
       " 'alpha47',\n",
       " 'alpha49',\n",
       " 'cap',\n",
       " 'alpha56',\n",
       " 'alpha57',\n",
       " 'alpha58',\n",
       " 'alpha51',\n",
       " 'alpha54',\n",
       " 'alpha60',\n",
       " 'adv180',\n",
       " 'adv120',\n",
       " 'adv60',\n",
       " 'adv50',\n",
       " 'adv15',\n",
       " 'alpha61',\n",
       " 'alpha62',\n",
       " 'alpha64',\n",
       " 'alpha65',\n",
       " 'alpha67',\n",
       " 'alpha68',\n",
       " 'alpha70',\n",
       " 'adv5',\n",
       " 'adv10',\n",
       " 'adv30',\n",
       " 'adv40',\n",
       " 'adv81',\n",
       " 'adv150',\n",
       " 'alpha73',\n",
       " 'alpha74',\n",
       " 'alpha75',\n",
       " 'alpha76',\n",
       " 'alpha77',\n",
       " 'alpha79',\n",
       " 'alpha81',\n",
       " 'alpha83',\n",
       " 'alpha84',\n",
       " 'alpha86',\n",
       " 'alpha87',\n",
       " 'alpha90',\n",
       " 'alpha92',\n",
       " 'alpha93',\n",
       " 'alpha98',\n",
       " 'alpha99',\n",
       " 'alpha100',\n",
       " 'alpha101',\n",
       " 'next_day_close',\n",
       " 'change',\n",
       " 'target']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc35d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 날짜 처리\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# 2. 정수형으로 처리할 컬럼 (이진 라벨 등)\n",
    "int_cols = ['target']\n",
    "\n",
    "# 3. float로 처리할 컬럼 (Date 제외 전부 float으로 변환 시도 후 int는 다시 변환)\n",
    "for col in df.columns:\n",
    "    if col != 'Date':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 4. 마지막에 명시적으로 int로 변환할 컬럼만 처리\n",
    "for col in int_cols:\n",
    "    df[col] = df[col].astype('Int64')  # NaN 허용 정수형\n",
    "\n",
    "# 선택: 'target'이 NaN 없이 0/1이면 그냥 int로도 가능\n",
    "# df['target'] = df['target'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cbc41d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>중국환율_시가</th>\n",
       "      <th>중국환율_고가</th>\n",
       "      <th>중국환율_저가</th>\n",
       "      <th>중국환율_종가</th>\n",
       "      <th>중국환율_변동%</th>\n",
       "      <th>WTI유_종가</th>\n",
       "      <th>WTI유_시가</th>\n",
       "      <th>WTI유_고가</th>\n",
       "      <th>WTI유_저가</th>\n",
       "      <th>...</th>\n",
       "      <th>alpha90</th>\n",
       "      <th>alpha92</th>\n",
       "      <th>alpha93</th>\n",
       "      <th>alpha98</th>\n",
       "      <th>alpha99</th>\n",
       "      <th>alpha100</th>\n",
       "      <th>alpha101</th>\n",
       "      <th>next_day_close</th>\n",
       "      <th>change</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>197.01</td>\n",
       "      <td>197.27</td>\n",
       "      <td>195.32</td>\n",
       "      <td>195.93</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>58.21</td>\n",
       "      <td>60.16</td>\n",
       "      <td>60.43</td>\n",
       "      <td>57.91</td>\n",
       "      <td>...</td>\n",
       "      <td>-847.000000</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-2795.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.533027</td>\n",
       "      <td>-0.553562</td>\n",
       "      <td>197.01</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>197.37</td>\n",
       "      <td>198.19</td>\n",
       "      <td>196.68</td>\n",
       "      <td>196.97</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>62.05</td>\n",
       "      <td>63.49</td>\n",
       "      <td>63.92</td>\n",
       "      <td>61.48</td>\n",
       "      <td>...</td>\n",
       "      <td>-750.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>-2467.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.880029</td>\n",
       "      <td>-0.264725</td>\n",
       "      <td>197.37</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>196.34</td>\n",
       "      <td>198.16</td>\n",
       "      <td>196.23</td>\n",
       "      <td>197.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.86</td>\n",
       "      <td>63.41</td>\n",
       "      <td>61.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>-1461.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.105074</td>\n",
       "      <td>0.533402</td>\n",
       "      <td>196.35</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>195.89</td>\n",
       "      <td>197.32</td>\n",
       "      <td>195.65</td>\n",
       "      <td>196.35</td>\n",
       "      <td>0.23</td>\n",
       "      <td>62.79</td>\n",
       "      <td>62.34</td>\n",
       "      <td>63.31</td>\n",
       "      <td>61.99</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>-1055.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052260</td>\n",
       "      <td>0.275284</td>\n",
       "      <td>195.89</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-23</td>\n",
       "      <td>195.59</td>\n",
       "      <td>196.05</td>\n",
       "      <td>194.70</td>\n",
       "      <td>195.89</td>\n",
       "      <td>0.15</td>\n",
       "      <td>62.27</td>\n",
       "      <td>64.00</td>\n",
       "      <td>64.87</td>\n",
       "      <td>61.53</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>-486.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.440872</td>\n",
       "      <td>0.222058</td>\n",
       "      <td>195.59</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>2009-10-16</td>\n",
       "      <td>169.48</td>\n",
       "      <td>170.99</td>\n",
       "      <td>169.02</td>\n",
       "      <td>169.21</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>78.53</td>\n",
       "      <td>77.76</td>\n",
       "      <td>78.75</td>\n",
       "      <td>76.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-92.833333</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>-1839.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.791072</td>\n",
       "      <td>-0.136986</td>\n",
       "      <td>169.61</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>2009-10-15</td>\n",
       "      <td>169.72</td>\n",
       "      <td>170.38</td>\n",
       "      <td>168.48</td>\n",
       "      <td>169.61</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>77.58</td>\n",
       "      <td>75.34</td>\n",
       "      <td>77.97</td>\n",
       "      <td>74.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-144.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>-1665.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.120217</td>\n",
       "      <td>-0.057864</td>\n",
       "      <td>169.71</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>2009-10-14</td>\n",
       "      <td>170.94</td>\n",
       "      <td>171.81</td>\n",
       "      <td>169.35</td>\n",
       "      <td>169.71</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>75.18</td>\n",
       "      <td>74.40</td>\n",
       "      <td>75.53</td>\n",
       "      <td>74.40</td>\n",
       "      <td>...</td>\n",
       "      <td>-318.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>-1176.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.952238</td>\n",
       "      <td>-0.499797</td>\n",
       "      <td>171.68</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>2009-10-13</td>\n",
       "      <td>170.95</td>\n",
       "      <td>171.68</td>\n",
       "      <td>170.71</td>\n",
       "      <td>171.68</td>\n",
       "      <td>0.30</td>\n",
       "      <td>74.15</td>\n",
       "      <td>73.17</td>\n",
       "      <td>74.55</td>\n",
       "      <td>72.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-3636.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>-775.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.579261</td>\n",
       "      <td>0.751802</td>\n",
       "      <td>171.17</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>2009-10-09</td>\n",
       "      <td>170.42</td>\n",
       "      <td>171.28</td>\n",
       "      <td>170.13</td>\n",
       "      <td>170.66</td>\n",
       "      <td>0.05</td>\n",
       "      <td>71.77</td>\n",
       "      <td>71.42</td>\n",
       "      <td>72.35</td>\n",
       "      <td>70.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-724.000000</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>-54.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.525106</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>170.58</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2760 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  중국환율_시가  중국환율_고가  중국환율_저가  중국환율_종가  중국환율_변동%  WTI유_종가  \\\n",
       "0    2025-04-30   197.01   197.27   195.32   195.93     -0.55    58.21   \n",
       "1    2025-04-28   197.37   198.19   196.68   196.97     -0.20    62.05   \n",
       "2    2025-04-25   196.34   198.16   196.23   197.37      0.52    63.02   \n",
       "3    2025-04-24   195.89   197.32   195.65   196.35      0.23    62.79   \n",
       "4    2025-04-23   195.59   196.05   194.70   195.89      0.15    62.27   \n",
       "...         ...      ...      ...      ...      ...       ...      ...   \n",
       "2755 2009-10-16   169.48   170.99   169.02   169.21     -0.24    78.53   \n",
       "2756 2009-10-15   169.72   170.38   168.48   169.61     -0.06    77.58   \n",
       "2757 2009-10-14   170.94   171.81   169.35   169.71     -1.15    75.18   \n",
       "2758 2009-10-13   170.95   171.68   170.71   171.68      0.30    74.15   \n",
       "2759 2009-10-09   170.42   171.28   170.13   170.66      0.05    71.77   \n",
       "\n",
       "      WTI유_시가  WTI유_고가  WTI유_저가  ...      alpha90   alpha92   alpha93  \\\n",
       "0       60.16    60.43    57.91  ...  -847.000000  0.305556  0.000059   \n",
       "1       63.49    63.92    61.48  ...  -750.166667  0.055556  0.000268   \n",
       "2       62.86    63.41    61.80  ... -1212.166667  0.055556  0.000482   \n",
       "3       62.34    63.31    61.99  ... -1212.166667  0.111111  0.000994   \n",
       "4       64.00    64.87    61.53  ... -1212.166667  0.166667  0.002033   \n",
       "...       ...      ...      ...  ...          ...       ...       ...   \n",
       "2755    77.76    78.75    76.82  ...   -92.833333  0.055556  0.002801   \n",
       "2756    75.34    77.97    74.79  ...  -144.166667  0.083333  0.002950   \n",
       "2757    74.40    75.53    74.40  ...  -318.000000  0.166667  0.001805   \n",
       "2758    73.17    74.55    72.83  ... -3636.500000  0.250000  0.001289   \n",
       "2759    71.42    72.35    70.62  ...  -724.000000  0.638889  0.000613   \n",
       "\n",
       "      alpha98  alpha99  alpha100  alpha101  next_day_close  change  target  \n",
       "0     -2795.5        0  1.533027 -0.553562          197.01    1.08       0  \n",
       "1     -2467.5        0  1.880029 -0.264725          197.37    0.40       0  \n",
       "2     -1461.0        0 -2.105074  0.533402          196.35   -1.02       0  \n",
       "3     -1055.0        0  0.052260  0.275284          195.89   -0.46       0  \n",
       "4      -486.0        0 -3.440872  0.222058          195.59   -0.30       0  \n",
       "...       ...      ...       ...       ...             ...     ...     ...  \n",
       "2755  -1839.5       -1 -0.791072 -0.136986          169.61    0.40       0  \n",
       "2756  -1665.5        0 -2.120217 -0.057864          169.71    0.10       0  \n",
       "2757  -1176.5        0 -0.952238 -0.499797          171.68    1.97       0  \n",
       "2758   -775.0        0 -2.579261  0.751802          171.17   -0.51       0  \n",
       "2759    -54.5       -1 -1.525106  0.208514          170.58   -0.08       0  \n",
       "\n",
       "[2760 rows x 204 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e214057",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4eca542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2760 entries, 0 to 2759\n",
      "Columns: 203 entries, Date to target\n",
      "dtypes: Int64(1), bool(2), datetime64[ns](1), float64(184), int64(15)\n",
      "memory usage: 4.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28829227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('위안화최종데이터.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c54cf96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.sum of 0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "2753    0\n",
       "2754    0\n",
       "2755    0\n",
       "2756    0\n",
       "2757    0\n",
       "Name: target, Length: 2758, dtype: int32>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b252831-1b5b-478d-83ca-1a82f8691f01",
   "metadata": {},
   "source": [
    "# 하락 / 상승및보합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1df74083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('위안화최종데이터.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06462733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>중국환율_시가</th>\n",
       "      <th>중국환율_고가</th>\n",
       "      <th>중국환율_저가</th>\n",
       "      <th>중국환율_종가</th>\n",
       "      <th>중국환율_변동%</th>\n",
       "      <th>WTI유_종가</th>\n",
       "      <th>WTI유_시가</th>\n",
       "      <th>WTI유_고가</th>\n",
       "      <th>WTI유_저가</th>\n",
       "      <th>...</th>\n",
       "      <th>alpha90</th>\n",
       "      <th>alpha92</th>\n",
       "      <th>alpha93</th>\n",
       "      <th>alpha98</th>\n",
       "      <th>alpha99</th>\n",
       "      <th>alpha100</th>\n",
       "      <th>alpha101</th>\n",
       "      <th>next_day_close</th>\n",
       "      <th>change</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>197.01</td>\n",
       "      <td>197.27</td>\n",
       "      <td>195.32</td>\n",
       "      <td>195.93</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>58.21</td>\n",
       "      <td>60.16</td>\n",
       "      <td>60.43</td>\n",
       "      <td>57.91</td>\n",
       "      <td>...</td>\n",
       "      <td>-847.000000</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-2795.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.533027</td>\n",
       "      <td>-0.553562</td>\n",
       "      <td>197.01</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>197.37</td>\n",
       "      <td>198.19</td>\n",
       "      <td>196.68</td>\n",
       "      <td>196.97</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>62.05</td>\n",
       "      <td>63.49</td>\n",
       "      <td>63.92</td>\n",
       "      <td>61.48</td>\n",
       "      <td>...</td>\n",
       "      <td>-750.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>-2467.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.880029</td>\n",
       "      <td>-0.264725</td>\n",
       "      <td>197.37</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>196.34</td>\n",
       "      <td>198.16</td>\n",
       "      <td>196.23</td>\n",
       "      <td>197.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.86</td>\n",
       "      <td>63.41</td>\n",
       "      <td>61.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>-1461.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.105074</td>\n",
       "      <td>0.533402</td>\n",
       "      <td>196.35</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>195.89</td>\n",
       "      <td>197.32</td>\n",
       "      <td>195.65</td>\n",
       "      <td>196.35</td>\n",
       "      <td>0.23</td>\n",
       "      <td>62.79</td>\n",
       "      <td>62.34</td>\n",
       "      <td>63.31</td>\n",
       "      <td>61.99</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>-1055.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052260</td>\n",
       "      <td>0.275284</td>\n",
       "      <td>195.89</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-23</td>\n",
       "      <td>195.59</td>\n",
       "      <td>196.05</td>\n",
       "      <td>194.70</td>\n",
       "      <td>195.89</td>\n",
       "      <td>0.15</td>\n",
       "      <td>62.27</td>\n",
       "      <td>64.00</td>\n",
       "      <td>64.87</td>\n",
       "      <td>61.53</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>-486.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.440872</td>\n",
       "      <td>0.222058</td>\n",
       "      <td>195.59</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>2009-10-16</td>\n",
       "      <td>169.48</td>\n",
       "      <td>170.99</td>\n",
       "      <td>169.02</td>\n",
       "      <td>169.21</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>78.53</td>\n",
       "      <td>77.76</td>\n",
       "      <td>78.75</td>\n",
       "      <td>76.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-92.833333</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>-1839.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.791072</td>\n",
       "      <td>-0.136986</td>\n",
       "      <td>169.61</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>2009-10-15</td>\n",
       "      <td>169.72</td>\n",
       "      <td>170.38</td>\n",
       "      <td>168.48</td>\n",
       "      <td>169.61</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>77.58</td>\n",
       "      <td>75.34</td>\n",
       "      <td>77.97</td>\n",
       "      <td>74.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-144.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>-1665.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.120217</td>\n",
       "      <td>-0.057864</td>\n",
       "      <td>169.71</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>2009-10-14</td>\n",
       "      <td>170.94</td>\n",
       "      <td>171.81</td>\n",
       "      <td>169.35</td>\n",
       "      <td>169.71</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>75.18</td>\n",
       "      <td>74.40</td>\n",
       "      <td>75.53</td>\n",
       "      <td>74.40</td>\n",
       "      <td>...</td>\n",
       "      <td>-318.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>-1176.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.952238</td>\n",
       "      <td>-0.499797</td>\n",
       "      <td>171.68</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>2009-10-13</td>\n",
       "      <td>170.95</td>\n",
       "      <td>171.68</td>\n",
       "      <td>170.71</td>\n",
       "      <td>171.68</td>\n",
       "      <td>0.30</td>\n",
       "      <td>74.15</td>\n",
       "      <td>73.17</td>\n",
       "      <td>74.55</td>\n",
       "      <td>72.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-3636.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>-775.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.579261</td>\n",
       "      <td>0.751802</td>\n",
       "      <td>171.17</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>2009-10-09</td>\n",
       "      <td>170.42</td>\n",
       "      <td>171.28</td>\n",
       "      <td>170.13</td>\n",
       "      <td>170.66</td>\n",
       "      <td>0.05</td>\n",
       "      <td>71.77</td>\n",
       "      <td>71.42</td>\n",
       "      <td>72.35</td>\n",
       "      <td>70.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-724.000000</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>-54.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.525106</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>170.58</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2760 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  중국환율_시가  중국환율_고가  중국환율_저가  중국환율_종가  중국환율_변동%  WTI유_종가  \\\n",
       "0     2025-04-30   197.01   197.27   195.32   195.93     -0.55    58.21   \n",
       "1     2025-04-28   197.37   198.19   196.68   196.97     -0.20    62.05   \n",
       "2     2025-04-25   196.34   198.16   196.23   197.37      0.52    63.02   \n",
       "3     2025-04-24   195.89   197.32   195.65   196.35      0.23    62.79   \n",
       "4     2025-04-23   195.59   196.05   194.70   195.89      0.15    62.27   \n",
       "...          ...      ...      ...      ...      ...       ...      ...   \n",
       "2755  2009-10-16   169.48   170.99   169.02   169.21     -0.24    78.53   \n",
       "2756  2009-10-15   169.72   170.38   168.48   169.61     -0.06    77.58   \n",
       "2757  2009-10-14   170.94   171.81   169.35   169.71     -1.15    75.18   \n",
       "2758  2009-10-13   170.95   171.68   170.71   171.68      0.30    74.15   \n",
       "2759  2009-10-09   170.42   171.28   170.13   170.66      0.05    71.77   \n",
       "\n",
       "      WTI유_시가  WTI유_고가  WTI유_저가  ...      alpha90   alpha92   alpha93  \\\n",
       "0       60.16    60.43    57.91  ...  -847.000000  0.305556  0.000059   \n",
       "1       63.49    63.92    61.48  ...  -750.166667  0.055556  0.000268   \n",
       "2       62.86    63.41    61.80  ... -1212.166667  0.055556  0.000482   \n",
       "3       62.34    63.31    61.99  ... -1212.166667  0.111111  0.000994   \n",
       "4       64.00    64.87    61.53  ... -1212.166667  0.166667  0.002033   \n",
       "...       ...      ...      ...  ...          ...       ...       ...   \n",
       "2755    77.76    78.75    76.82  ...   -92.833333  0.055556  0.002801   \n",
       "2756    75.34    77.97    74.79  ...  -144.166667  0.083333  0.002950   \n",
       "2757    74.40    75.53    74.40  ...  -318.000000  0.166667  0.001805   \n",
       "2758    73.17    74.55    72.83  ... -3636.500000  0.250000  0.001289   \n",
       "2759    71.42    72.35    70.62  ...  -724.000000  0.638889  0.000613   \n",
       "\n",
       "      alpha98  alpha99  alpha100  alpha101  next_day_close  change  target  \n",
       "0     -2795.5        0  1.533027 -0.553562          197.01    1.08       0  \n",
       "1     -2467.5        0  1.880029 -0.264725          197.37    0.40       0  \n",
       "2     -1461.0        0 -2.105074  0.533402          196.35   -1.02       0  \n",
       "3     -1055.0        0  0.052260  0.275284          195.89   -0.46       0  \n",
       "4      -486.0        0 -3.440872  0.222058          195.59   -0.30       0  \n",
       "...       ...      ...       ...       ...             ...     ...     ...  \n",
       "2755  -1839.5       -1 -0.791072 -0.136986          169.61    0.40       0  \n",
       "2756  -1665.5        0 -2.120217 -0.057864          169.71    0.10       0  \n",
       "2757  -1176.5        0 -0.952238 -0.499797          171.68    1.97       0  \n",
       "2758   -775.0        0 -2.579261  0.751802          171.17   -0.51       0  \n",
       "2759    -54.5       -1 -1.525106  0.208514          170.58   -0.08       0  \n",
       "\n",
       "[2760 rows x 204 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95b48fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>중국환율_시가</th>\n",
       "      <th>중국환율_고가</th>\n",
       "      <th>중국환율_저가</th>\n",
       "      <th>중국환율_종가</th>\n",
       "      <th>중국환율_변동%</th>\n",
       "      <th>WTI유_종가</th>\n",
       "      <th>WTI유_시가</th>\n",
       "      <th>WTI유_고가</th>\n",
       "      <th>WTI유_저가</th>\n",
       "      <th>...</th>\n",
       "      <th>alpha90</th>\n",
       "      <th>alpha92</th>\n",
       "      <th>alpha93</th>\n",
       "      <th>alpha98</th>\n",
       "      <th>alpha99</th>\n",
       "      <th>alpha100</th>\n",
       "      <th>alpha101</th>\n",
       "      <th>next_day_close</th>\n",
       "      <th>change</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, 중국환율_시가, 중국환율_고가, 중국환율_저가, 중국환율_종가, 중국환율_변동%, WTI유_종가, WTI유_시가, WTI유_고가, WTI유_저가, WTI유_거래량, WTI유_변동%, 금_종가, 금_시가, 금_고가, 금_저가, 금_거래량, 금_변동%, S&P500_종가, S&P500_시가, S&P500_고가, S&P500_저가, S&P500_변동%, 다우존스_종가, 다우존스_시가, 다우존스_고가, 다우존스_저가, 다우존스_거래량, 다우존스_변동%, 상해종합_종가, 상해종합_시가, 상해종합_고가, 상해종합_저가, 상해종합_거래량, 상해종합_변동%, 닛케이_종가, 닛케이_시가, 닛케이_고가, 닛케이_저가, 닛케이_변동%, 코스피_종가, 코스피_시가, 코스피_고가, 코스피_저가, 코스피_거래량, 코스피_변동%, 나스닥_종가, 나스닥_시가, 나스닥_고가, 나스닥_저가, 나스닥_거래량, 나스닥_변동%, VIX_종가, VIX_시가, VIX_고가, VIX_저가, VIX_거래량, VIX_변동%, 시가총액_전체, 시가총액_외국인보유, 시가총액_비율, 주식수_전체, 주식수_외국인보유, 주식수_비율, CD금리(91일), 국고채(3년), 한국정책금리, 미국정책금리, 기준년월, 한국(M1)조원, 한국(M1)변동%, 한국(M2)조원, 한국(M2)변동%, 미국(M1)십억달러, 미국(M2)십억달러, 소비자심리지수, 생산자물가지수, 산업생산지수, 외환보유액(억달러), 경상수지, 미국소비자물가지수, 비트코인_검색량, 일본엔_검색량, 유로(EUR)_검색량, S&P 500_검색량, 코스피_검색량, 일본EPU, 중국EPU, 한국EPU, 미국EPU, 글로벌EPU_명목GDP기준, 글로벌EPU_PPP기준, return, log_return, lag_1, lag_2, lag_3, lag_4, lag_5, sma_5, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 204 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# ✅ 하이퍼파라미터 범위 설정\n",
    "change_cut = 0.7\n",
    "alpha_list = [0.49, 0.5]       # 성능 중심 구간\n",
    "threshold_list = [0.49, 0.5]   # 유효 범위 압축\n",
    "weight_list = [10, 11]     # class_weight_1 \n",
    "\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] <= -change_cut).astype(int)\n",
    "df.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f047a670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>중국환율_시가</th>\n",
       "      <th>중국환율_고가</th>\n",
       "      <th>중국환율_저가</th>\n",
       "      <th>중국환율_종가</th>\n",
       "      <th>중국환율_변동%</th>\n",
       "      <th>WTI유_종가</th>\n",
       "      <th>WTI유_시가</th>\n",
       "      <th>WTI유_고가</th>\n",
       "      <th>WTI유_저가</th>\n",
       "      <th>...</th>\n",
       "      <th>alpha90</th>\n",
       "      <th>alpha92</th>\n",
       "      <th>alpha93</th>\n",
       "      <th>alpha98</th>\n",
       "      <th>alpha99</th>\n",
       "      <th>alpha100</th>\n",
       "      <th>alpha101</th>\n",
       "      <th>next_day_close</th>\n",
       "      <th>change</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2760 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date  중국환율_시가  중국환율_고가  중국환율_저가  중국환율_종가  중국환율_변동%  WTI유_종가  WTI유_시가  \\\n",
       "0     False    False    False    False    False     False    False    False   \n",
       "1     False    False    False    False    False     False    False    False   \n",
       "2     False    False    False    False    False     False    False    False   \n",
       "3     False    False    False    False    False     False    False    False   \n",
       "4     False    False    False    False    False     False    False    False   \n",
       "...     ...      ...      ...      ...      ...       ...      ...      ...   \n",
       "2755  False    False    False    False    False     False    False    False   \n",
       "2756  False    False    False    False    False     False    False    False   \n",
       "2757  False    False    False    False    False     False    False    False   \n",
       "2758  False    False    False    False    False     False    False    False   \n",
       "2759  False    False    False    False    False     False    False    False   \n",
       "\n",
       "      WTI유_고가  WTI유_저가  ...  alpha90  alpha92  alpha93  alpha98  alpha99  \\\n",
       "0       False    False  ...    False    False    False    False    False   \n",
       "1       False    False  ...    False    False    False    False    False   \n",
       "2       False    False  ...    False    False    False    False    False   \n",
       "3       False    False  ...    False    False    False    False    False   \n",
       "4       False    False  ...    False    False    False    False    False   \n",
       "...       ...      ...  ...      ...      ...      ...      ...      ...   \n",
       "2755    False    False  ...    False    False    False    False    False   \n",
       "2756    False    False  ...    False    False    False    False    False   \n",
       "2757    False    False  ...    False    False    False    False    False   \n",
       "2758    False    False  ...    False    False    False    False    False   \n",
       "2759    False    False  ...    False    False    False    False    False   \n",
       "\n",
       "      alpha100  alpha101  next_day_close  change  target  \n",
       "0        False     False           False   False   False  \n",
       "1        False     False           False   False   False  \n",
       "2        False     False           False   False   False  \n",
       "3        False     False           False   False   False  \n",
       "4        False     False           False   False   False  \n",
       "...        ...       ...             ...     ...     ...  \n",
       "2755     False     False           False   False   False  \n",
       "2756     False     False           False   False   False  \n",
       "2757     False     False           False   False   False  \n",
       "2758     False     False           False   False   False  \n",
       "2759     False     False            True    True   False  \n",
       "\n",
       "[2760 rows x 204 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d3ce8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       195.93\n",
       "1       196.97\n",
       "2       197.37\n",
       "3       196.35\n",
       "4       195.89\n",
       "         ...  \n",
       "2755    169.21\n",
       "2756    169.61\n",
       "2757    169.71\n",
       "2758    171.68\n",
       "2759    170.66\n",
       "Name: 중국환율_종가, Length: 2760, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['중국환율_종가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d53000a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>중국환율_시가</th>\n",
       "      <th>중국환율_고가</th>\n",
       "      <th>중국환율_저가</th>\n",
       "      <th>중국환율_종가</th>\n",
       "      <th>중국환율_변동%</th>\n",
       "      <th>WTI유_종가</th>\n",
       "      <th>WTI유_시가</th>\n",
       "      <th>WTI유_고가</th>\n",
       "      <th>WTI유_저가</th>\n",
       "      <th>...</th>\n",
       "      <th>alpha90</th>\n",
       "      <th>alpha92</th>\n",
       "      <th>alpha93</th>\n",
       "      <th>alpha98</th>\n",
       "      <th>alpha99</th>\n",
       "      <th>alpha100</th>\n",
       "      <th>alpha101</th>\n",
       "      <th>next_day_close</th>\n",
       "      <th>change</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>197.01</td>\n",
       "      <td>197.27</td>\n",
       "      <td>195.32</td>\n",
       "      <td>195.93</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>58.21</td>\n",
       "      <td>60.16</td>\n",
       "      <td>60.43</td>\n",
       "      <td>57.91</td>\n",
       "      <td>...</td>\n",
       "      <td>-847.000000</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-2795.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.533027</td>\n",
       "      <td>-0.553562</td>\n",
       "      <td>197.01</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>197.37</td>\n",
       "      <td>198.19</td>\n",
       "      <td>196.68</td>\n",
       "      <td>196.97</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>62.05</td>\n",
       "      <td>63.49</td>\n",
       "      <td>63.92</td>\n",
       "      <td>61.48</td>\n",
       "      <td>...</td>\n",
       "      <td>-750.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>-2467.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.880029</td>\n",
       "      <td>-0.264725</td>\n",
       "      <td>197.37</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>196.34</td>\n",
       "      <td>198.16</td>\n",
       "      <td>196.23</td>\n",
       "      <td>197.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.86</td>\n",
       "      <td>63.41</td>\n",
       "      <td>61.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>-1461.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.105074</td>\n",
       "      <td>0.533402</td>\n",
       "      <td>196.35</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>195.89</td>\n",
       "      <td>197.32</td>\n",
       "      <td>195.65</td>\n",
       "      <td>196.35</td>\n",
       "      <td>0.23</td>\n",
       "      <td>62.79</td>\n",
       "      <td>62.34</td>\n",
       "      <td>63.31</td>\n",
       "      <td>61.99</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>-1055.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052260</td>\n",
       "      <td>0.275284</td>\n",
       "      <td>195.89</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-23</td>\n",
       "      <td>195.59</td>\n",
       "      <td>196.05</td>\n",
       "      <td>194.70</td>\n",
       "      <td>195.89</td>\n",
       "      <td>0.15</td>\n",
       "      <td>62.27</td>\n",
       "      <td>64.00</td>\n",
       "      <td>64.87</td>\n",
       "      <td>61.53</td>\n",
       "      <td>...</td>\n",
       "      <td>-1212.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>-486.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.440872</td>\n",
       "      <td>0.222058</td>\n",
       "      <td>195.59</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>2009-10-16</td>\n",
       "      <td>169.48</td>\n",
       "      <td>170.99</td>\n",
       "      <td>169.02</td>\n",
       "      <td>169.21</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>78.53</td>\n",
       "      <td>77.76</td>\n",
       "      <td>78.75</td>\n",
       "      <td>76.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-92.833333</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>-1839.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.791072</td>\n",
       "      <td>-0.136986</td>\n",
       "      <td>169.61</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>2009-10-15</td>\n",
       "      <td>169.72</td>\n",
       "      <td>170.38</td>\n",
       "      <td>168.48</td>\n",
       "      <td>169.61</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>77.58</td>\n",
       "      <td>75.34</td>\n",
       "      <td>77.97</td>\n",
       "      <td>74.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-144.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>-1665.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.120217</td>\n",
       "      <td>-0.057864</td>\n",
       "      <td>169.71</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>2009-10-14</td>\n",
       "      <td>170.94</td>\n",
       "      <td>171.81</td>\n",
       "      <td>169.35</td>\n",
       "      <td>169.71</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>75.18</td>\n",
       "      <td>74.40</td>\n",
       "      <td>75.53</td>\n",
       "      <td>74.40</td>\n",
       "      <td>...</td>\n",
       "      <td>-318.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>-1176.5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.952238</td>\n",
       "      <td>-0.499797</td>\n",
       "      <td>171.68</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>2009-10-13</td>\n",
       "      <td>170.95</td>\n",
       "      <td>171.68</td>\n",
       "      <td>170.71</td>\n",
       "      <td>171.68</td>\n",
       "      <td>0.30</td>\n",
       "      <td>74.15</td>\n",
       "      <td>73.17</td>\n",
       "      <td>74.55</td>\n",
       "      <td>72.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-3636.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>-775.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.579261</td>\n",
       "      <td>0.751802</td>\n",
       "      <td>171.17</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>2009-10-09</td>\n",
       "      <td>170.42</td>\n",
       "      <td>171.28</td>\n",
       "      <td>170.13</td>\n",
       "      <td>170.66</td>\n",
       "      <td>0.05</td>\n",
       "      <td>71.77</td>\n",
       "      <td>71.42</td>\n",
       "      <td>72.35</td>\n",
       "      <td>70.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-724.000000</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>-54.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.525106</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>170.58</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2760 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  중국환율_시가  중국환율_고가  중국환율_저가  중국환율_종가  중국환율_변동%  WTI유_종가  \\\n",
       "0     2025-04-30   197.01   197.27   195.32   195.93     -0.55    58.21   \n",
       "1     2025-04-28   197.37   198.19   196.68   196.97     -0.20    62.05   \n",
       "2     2025-04-25   196.34   198.16   196.23   197.37      0.52    63.02   \n",
       "3     2025-04-24   195.89   197.32   195.65   196.35      0.23    62.79   \n",
       "4     2025-04-23   195.59   196.05   194.70   195.89      0.15    62.27   \n",
       "...          ...      ...      ...      ...      ...       ...      ...   \n",
       "2755  2009-10-16   169.48   170.99   169.02   169.21     -0.24    78.53   \n",
       "2756  2009-10-15   169.72   170.38   168.48   169.61     -0.06    77.58   \n",
       "2757  2009-10-14   170.94   171.81   169.35   169.71     -1.15    75.18   \n",
       "2758  2009-10-13   170.95   171.68   170.71   171.68      0.30    74.15   \n",
       "2759  2009-10-09   170.42   171.28   170.13   170.66      0.05    71.77   \n",
       "\n",
       "      WTI유_시가  WTI유_고가  WTI유_저가  ...      alpha90   alpha92   alpha93  \\\n",
       "0       60.16    60.43    57.91  ...  -847.000000  0.305556  0.000059   \n",
       "1       63.49    63.92    61.48  ...  -750.166667  0.055556  0.000268   \n",
       "2       62.86    63.41    61.80  ... -1212.166667  0.055556  0.000482   \n",
       "3       62.34    63.31    61.99  ... -1212.166667  0.111111  0.000994   \n",
       "4       64.00    64.87    61.53  ... -1212.166667  0.166667  0.002033   \n",
       "...       ...      ...      ...  ...          ...       ...       ...   \n",
       "2755    77.76    78.75    76.82  ...   -92.833333  0.055556  0.002801   \n",
       "2756    75.34    77.97    74.79  ...  -144.166667  0.083333  0.002950   \n",
       "2757    74.40    75.53    74.40  ...  -318.000000  0.166667  0.001805   \n",
       "2758    73.17    74.55    72.83  ... -3636.500000  0.250000  0.001289   \n",
       "2759    71.42    72.35    70.62  ...  -724.000000  0.638889  0.000613   \n",
       "\n",
       "      alpha98  alpha99  alpha100  alpha101  next_day_close  change  target  \n",
       "0     -2795.5        0  1.533027 -0.553562          197.01    1.08       0  \n",
       "1     -2467.5        0  1.880029 -0.264725          197.37    0.40       0  \n",
       "2     -1461.0        0 -2.105074  0.533402          196.35   -1.02       0  \n",
       "3     -1055.0        0  0.052260  0.275284          195.89   -0.46       0  \n",
       "4      -486.0        0 -3.440872  0.222058          195.59   -0.30       0  \n",
       "...       ...      ...       ...       ...             ...     ...     ...  \n",
       "2755  -1839.5       -1 -0.791072 -0.136986          169.61    0.40       0  \n",
       "2756  -1665.5        0 -2.120217 -0.057864          169.71    0.10       0  \n",
       "2757  -1176.5        0 -0.952238 -0.499797          171.68    1.97       0  \n",
       "2758   -775.0        0 -2.579261  0.751802          171.17   -0.51       0  \n",
       "2759    -54.5       -1 -1.525106  0.208514          170.58   -0.08       0  \n",
       "\n",
       "[2760 rows x 204 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b52f0239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>중국환율_시가</th>\n",
       "      <th>중국환율_고가</th>\n",
       "      <th>중국환율_저가</th>\n",
       "      <th>중국환율_종가</th>\n",
       "      <th>중국환율_변동%</th>\n",
       "      <th>WTI유_종가</th>\n",
       "      <th>WTI유_시가</th>\n",
       "      <th>WTI유_고가</th>\n",
       "      <th>WTI유_저가</th>\n",
       "      <th>...</th>\n",
       "      <th>alpha90</th>\n",
       "      <th>alpha92</th>\n",
       "      <th>alpha93</th>\n",
       "      <th>alpha98</th>\n",
       "      <th>alpha99</th>\n",
       "      <th>alpha100</th>\n",
       "      <th>alpha101</th>\n",
       "      <th>next_day_close</th>\n",
       "      <th>change</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, 중국환율_시가, 중국환율_고가, 중국환율_저가, 중국환율_종가, 중국환율_변동%, WTI유_종가, WTI유_시가, WTI유_고가, WTI유_저가, WTI유_거래량, WTI유_변동%, 금_종가, 금_시가, 금_고가, 금_저가, 금_거래량, 금_변동%, S&P500_종가, S&P500_시가, S&P500_고가, S&P500_저가, S&P500_변동%, 다우존스_종가, 다우존스_시가, 다우존스_고가, 다우존스_저가, 다우존스_거래량, 다우존스_변동%, 상해종합_종가, 상해종합_시가, 상해종합_고가, 상해종합_저가, 상해종합_거래량, 상해종합_변동%, 닛케이_종가, 닛케이_시가, 닛케이_고가, 닛케이_저가, 닛케이_변동%, 코스피_종가, 코스피_시가, 코스피_고가, 코스피_저가, 코스피_거래량, 코스피_변동%, 나스닥_종가, 나스닥_시가, 나스닥_고가, 나스닥_저가, 나스닥_거래량, 나스닥_변동%, VIX_종가, VIX_시가, VIX_고가, VIX_저가, VIX_거래량, VIX_변동%, 시가총액_전체, 시가총액_외국인보유, 시가총액_비율, 주식수_전체, 주식수_외국인보유, 주식수_비율, CD금리(91일), 국고채(3년), 한국정책금리, 미국정책금리, 기준년월, 한국(M1)조원, 한국(M1)변동%, 한국(M2)조원, 한국(M2)변동%, 미국(M1)십억달러, 미국(M2)십억달러, 소비자심리지수, 생산자물가지수, 산업생산지수, 외환보유액(억달러), 경상수지, 미국소비자물가지수, 비트코인_검색량, 일본엔_검색량, 유로(EUR)_검색량, S&P 500_검색량, 코스피_검색량, 일본EPU, 중국EPU, 한국EPU, 미국EPU, 글로벌EPU_명목GDP기준, 글로벌EPU_PPP기준, return, log_return, lag_1, lag_2, lag_3, lag_4, lag_5, sma_5, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 204 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65333b0",
   "metadata": {},
   "source": [
    "## 아래 코드는 상승/하락 또는 보합 (상승에 보수적)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b7338aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0.7, alpha: 0.21, threshold: 0.4000, class_weight_1: 11.0, macro f1-score: 0.7601\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92       447\n",
      "           1       0.64      0.57      0.60       102\n",
      "\n",
      "    accuracy                           0.86       549\n",
      "   macro avg       0.77      0.75      0.76       549\n",
      "weighted avg       0.86      0.86      0.86       549\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[415  32]\n",
      " [ 44  58]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0.7\n",
    "alpha_list = np.round(np.arange(0.01, 1.0, 0.1), 2)       # 성능 중심 구간\n",
    "threshold_list = np.round(np.arange(0.1, 1.0, 0.1), 2)    # 유효 범위 압축\n",
    "weight_list = np.round(np.arange(6.0, 16.0, 1.0), 2)      # class_weight_1\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = LSTM(64, return_sequences=True)(inputs)\n",
    "x = LSTM(32)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fdbe9599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0.7, alpha: 0.21, threshold: 0.4000, class_weight_1: 11.0, macro f1-score: 0.7586\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92       447\n",
      "           1       0.70      0.52      0.60       102\n",
      "\n",
      "    accuracy                           0.87       549\n",
      "   macro avg       0.80      0.73      0.76       549\n",
      "weighted avg       0.86      0.87      0.86       549\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[424  23]\n",
      " [ 49  53]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0.7\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "# ✅ 수동으로 최적 조합 지정\n",
    "best_alpha = 0.21\n",
    "best_threshold = 0.4\n",
    "best_weight = 11.0\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = LSTM(64, return_sequences=True)(inputs)\n",
    "x = LSTM(32)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea231517-1b74-49dd-8440-28c2e61b9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = np.round(np.arange(0.01, 1.0, 0.1), 2)       # 성능 중심 구간\n",
    "threshold_list = np.round(np.arange(0.1, 1.0, 0.1), 2)    # 유효 범위 압축\n",
    "weight_list = np.round(np.arange(6.0, 16.0, 1.0), 2)      # class_weight_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if isinstance(df[col], pd.Series) and df[col].apply(lambda x: isinstance(x, pd.Series)).any():\n",
    "        df[col] = df[col].apply(lambda x: x.squeeze() if isinstance(x, pd.Series) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0968b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:   1%|          | 1/100 [00:14<24:03, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 1/100개 완료 | 이번에 걸린 시간: 14.6s | 예상 남은 시간: 24.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:   2%|▏         | 2/100 [00:30<25:27, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 2/100개 완료 | 이번에 걸린 시간: 16.3s | 예상 남은 시간: 25.2분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:   3%|▎         | 3/100 [00:48<26:30, 16.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 3/100개 완료 | 이번에 걸린 시간: 17.4s | 예상 남은 시간: 26.0분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:   4%|▍         | 4/100 [01:04<25:52, 16.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 4/100개 완료 | 이번에 걸린 시간: 15.8s | 예상 남은 시간: 25.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:   5%|▌         | 5/100 [01:23<27:30, 17.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 5/100개 완료 | 이번에 걸린 시간: 19.5s | 예상 남은 시간: 26.5분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:   6%|▌         | 6/100 [01:37<25:20, 16.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 6/100개 완료 | 이번에 걸린 시간: 13.8s | 예상 남은 시간: 25.4분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:   7%|▋         | 7/100 [01:54<25:30, 16.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 7/100개 완료 | 이번에 걸린 시간: 17.1s | 예상 남은 시간: 25.3분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:   8%|▊         | 8/100 [02:11<25:36, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 8/100개 완료 | 이번에 걸린 시간: 17.2s | 예상 남은 시간: 25.2분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:   9%|▉         | 9/100 [02:27<24:41, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 9/100개 완료 | 이번에 걸린 시간: 15.3s | 예상 남은 시간: 24.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  10%|█         | 10/100 [02:43<24:17, 16.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 10/100개 완료 | 이번에 걸린 시간: 16.0s | 예상 남은 시간: 24.5분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  11%|█         | 11/100 [02:58<23:28, 15.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 11/100개 완료 | 이번에 걸린 시간: 15.0s | 예상 남은 시간: 24.0분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  12%|█▏        | 12/100 [03:16<24:27, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 12/100개 완료 | 이번에 걸린 시간: 18.6s | 예상 남은 시간: 24.0분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  13%|█▎        | 13/100 [03:33<24:25, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 13/100개 완료 | 이번에 걸린 시간: 17.2s | 예상 남은 시간: 23.9분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  14%|█▍        | 14/100 [03:51<24:28, 17.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 14/100개 완료 | 이번에 걸린 시간: 17.6s | 예상 남은 시간: 23.7분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  15%|█▌        | 15/100 [04:07<23:32, 16.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 15/100개 완료 | 이번에 걸린 시간: 15.6s | 예상 남은 시간: 23.3분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  16%|█▌        | 16/100 [04:21<22:21, 15.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 16/100개 완료 | 이번에 걸린 시간: 14.4s | 예상 남은 시간: 22.9분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  17%|█▋        | 17/100 [04:37<22:07, 16.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 17/100개 완료 | 이번에 걸린 시간: 16.1s | 예상 남은 시간: 22.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  18%|█▊        | 18/100 [04:59<24:05, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 18/100개 완료 | 이번에 걸린 시간: 21.4s | 예상 남은 시간: 22.7분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  19%|█▉        | 19/100 [05:22<26:18, 19.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 19/100개 완료 | 이번에 걸린 시간: 23.8s | 예상 남은 시간: 22.9분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  20%|██        | 20/100 [05:43<26:29, 19.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 20/100개 완료 | 이번에 걸린 시간: 20.8s | 예상 남은 시간: 22.9분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  21%|██        | 21/100 [06:16<31:10, 23.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 21/100개 완료 | 이번에 걸린 시간: 32.6s | 예상 남은 시간: 23.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  22%|██▏       | 22/100 [06:42<31:38, 24.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 22/100개 완료 | 이번에 걸린 시간: 25.9s | 예상 남은 시간: 23.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  23%|██▎       | 23/100 [07:04<30:42, 23.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 23/100개 완료 | 이번에 걸린 시간: 23.0s | 예상 남은 시간: 23.7분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  24%|██▍       | 24/100 [07:28<29:58, 23.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 24/100개 완료 | 이번에 걸린 시간: 23.1s | 예상 남은 시간: 23.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  25%|██▌       | 25/100 [07:48<28:27, 22.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 25/100개 완료 | 이번에 걸린 시간: 20.7s | 예상 남은 시간: 23.4분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  26%|██▌       | 26/100 [08:16<29:57, 24.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 26/100개 완료 | 이번에 걸린 시간: 27.8s | 예상 남은 시간: 23.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  27%|██▋       | 27/100 [08:36<27:49, 22.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 27/100개 완료 | 이번에 걸린 시간: 19.6s | 예상 남은 시간: 23.3분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  28%|██▊       | 28/100 [08:58<27:05, 22.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 28/100개 완료 | 이번에 걸린 시간: 21.9s | 예상 남은 시간: 23.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  29%|██▉       | 29/100 [09:20<26:50, 22.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 29/100개 완료 | 이번에 걸린 시간: 22.9s | 예상 남은 시간: 22.9분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  30%|███       | 30/100 [09:41<25:35, 21.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 30/100개 완료 | 이번에 걸린 시간: 20.2s | 예상 남은 시간: 22.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  31%|███       | 31/100 [10:08<27:03, 23.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 31/100개 완료 | 이번에 걸린 시간: 27.2s | 예상 남은 시간: 22.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  32%|███▏      | 32/100 [10:29<25:47, 22.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 32/100개 완료 | 이번에 걸린 시간: 20.9s | 예상 남은 시간: 22.3분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  33%|███▎      | 33/100 [10:49<24:35, 22.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 33/100개 완료 | 이번에 걸린 시간: 20.3s | 예상 남은 시간: 22.0분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  34%|███▍      | 34/100 [11:19<26:50, 24.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 34/100개 완료 | 이번에 걸린 시간: 30.0s | 예상 남은 시간: 22.0분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  35%|███▌      | 35/100 [11:46<27:18, 25.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 35/100개 완료 | 이번에 걸린 시간: 27.1s | 예상 남은 시간: 21.9분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  36%|███▌      | 36/100 [12:17<28:37, 26.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 36/100개 완료 | 이번에 걸린 시간: 30.6s | 예상 남은 시간: 21.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  37%|███▋      | 37/100 [12:46<29:02, 27.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 37/100개 완료 | 이번에 걸린 시간: 29.6s | 예상 남은 시간: 21.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  38%|███▊      | 38/100 [13:14<28:29, 27.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 38/100개 완료 | 이번에 걸린 시간: 27.4s | 예상 남은 시간: 21.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  39%|███▉      | 39/100 [13:44<28:51, 28.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 39/100개 완료 | 이번에 걸린 시간: 30.2s | 예상 남은 시간: 21.5분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  40%|████      | 40/100 [14:10<27:44, 27.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 40/100개 완료 | 이번에 걸린 시간: 26.2s | 예상 남은 시간: 21.3분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  41%|████      | 41/100 [14:48<30:20, 30.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 41/100개 완료 | 이번에 걸린 시간: 38.1s | 예상 남은 시간: 21.3분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  42%|████▏     | 42/100 [15:24<31:05, 32.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 42/100개 완료 | 이번에 걸린 시간: 35.2s | 예상 남은 시간: 21.3분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  43%|████▎     | 43/100 [16:25<39:00, 41.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 43/100개 완료 | 이번에 걸린 시간: 61.8s | 예상 남은 시간: 21.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  44%|████▍     | 44/100 [17:25<43:33, 46.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 44/100개 완료 | 이번에 걸린 시간: 59.8s | 예상 남은 시간: 22.2분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  45%|████▌     | 45/100 [18:18<44:21, 48.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 45/100개 완료 | 이번에 걸린 시간: 52.4s | 예상 남은 시간: 22.4분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  46%|████▌     | 46/100 [19:09<44:27, 49.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 46/100개 완료 | 이번에 걸린 시간: 51.7s | 예상 남은 시간: 22.5분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  47%|████▋     | 47/100 [19:57<43:11, 48.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 47/100개 완료 | 이번에 걸린 시간: 47.7s | 예상 남은 시간: 22.5분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  48%|████▊     | 48/100 [21:00<46:07, 53.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 48/100개 완료 | 이번에 걸린 시간: 63.3s | 예상 남은 시간: 22.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  49%|████▉     | 49/100 [22:11<49:43, 58.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 49/100개 완료 | 이번에 걸린 시간: 70.8s | 예상 남은 시간: 23.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  50%|█████     | 50/100 [23:11<49:09, 58.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 50/100개 완료 | 이번에 걸린 시간: 60.1s | 예상 남은 시간: 23.2분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  51%|█████     | 51/100 [24:18<50:03, 61.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 51/100개 완료 | 이번에 걸린 시간: 66.7s | 예상 남은 시간: 23.4분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  52%|█████▏    | 52/100 [25:10<46:46, 58.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 52/100개 완료 | 이번에 걸린 시간: 51.8s | 예상 남은 시간: 23.2분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  53%|█████▎    | 53/100 [26:04<44:53, 57.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 53/100개 완료 | 이번에 걸린 시간: 54.6s | 예상 남은 시간: 23.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  54%|█████▍    | 54/100 [27:07<45:10, 58.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 54/100개 완료 | 이번에 걸린 시간: 62.7s | 예상 남은 시간: 23.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  55%|█████▌    | 55/100 [28:09<44:47, 59.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 55/100개 완료 | 이번에 걸린 시간: 61.6s | 예상 남은 시간: 23.0분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  56%|█████▌    | 56/100 [29:00<41:58, 57.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 56/100개 완료 | 이번에 걸린 시간: 51.4s | 예상 남은 시간: 22.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  57%|█████▋    | 57/100 [30:07<43:07, 60.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 57/100개 완료 | 이번에 걸린 시간: 67.0s | 예상 남은 시간: 22.7분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  58%|█████▊    | 58/100 [31:08<42:19, 60.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 58/100개 완료 | 이번에 걸린 시간: 61.2s | 예상 남은 시간: 22.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  59%|█████▉    | 59/100 [31:52<37:50, 55.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 59/100개 완료 | 이번에 걸린 시간: 43.4s | 예상 남은 시간: 22.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  60%|██████    | 60/100 [32:40<35:27, 53.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 60/100개 완료 | 이번에 걸린 시간: 48.1s | 예상 남은 시간: 21.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  61%|██████    | 61/100 [33:37<35:15, 54.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 61/100개 완료 | 이번에 걸린 시간: 56.7s | 예상 남은 시간: 21.5분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  62%|██████▏   | 62/100 [34:28<33:51, 53.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 62/100개 완료 | 이번에 걸린 시간: 51.6s | 예상 남은 시간: 21.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  63%|██████▎   | 63/100 [35:22<32:56, 53.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 63/100개 완료 | 이번에 걸린 시간: 53.3s | 예상 남은 시간: 20.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  64%|██████▍   | 64/100 [36:15<32:08, 53.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 64/100개 완료 | 이번에 걸린 시간: 53.9s | 예상 남은 시간: 20.4분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  65%|██████▌   | 65/100 [37:09<31:09, 53.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 65/100개 완료 | 이번에 걸린 시간: 53.0s | 예상 남은 시간: 20.0분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  66%|██████▌   | 66/100 [38:07<31:11, 55.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 66/100개 완료 | 이번에 걸린 시간: 58.8s | 예상 남은 시간: 19.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  67%|██████▋   | 67/100 [38:58<29:35, 53.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 67/100개 완료 | 이번에 걸린 시간: 51.0s | 예상 남은 시간: 19.2분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  68%|██████▊   | 68/100 [39:47<27:51, 52.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 68/100개 완료 | 이번에 걸린 시간: 48.5s | 예상 남은 시간: 18.7분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  69%|██████▉   | 69/100 [40:36<26:32, 51.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 69/100개 완료 | 이번에 걸린 시간: 49.4s | 예상 남은 시간: 18.2분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  70%|███████   | 70/100 [41:33<26:33, 53.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 70/100개 완료 | 이번에 걸린 시간: 57.2s | 예상 남은 시간: 17.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  71%|███████   | 71/100 [42:04<22:26, 46.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 71/100개 완료 | 이번에 걸린 시간: 30.8s | 예상 남은 시간: 17.2분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  72%|███████▏  | 72/100 [42:26<18:12, 39.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 72/100개 완료 | 이번에 걸린 시간: 21.7s | 예상 남은 시간: 16.5분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  73%|███████▎  | 73/100 [42:47<15:08, 33.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 73/100개 완료 | 이번에 걸린 시간: 21.1s | 예상 남은 시간: 15.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  74%|███████▍  | 74/100 [43:05<12:29, 28.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 74/100개 완료 | 이번에 걸린 시간: 17.5s | 예상 남은 시간: 15.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  75%|███████▌  | 75/100 [43:25<11:00, 26.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 75/100개 완료 | 이번에 걸린 시간: 20.8s | 예상 남은 시간: 14.5분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  76%|███████▌  | 76/100 [43:41<09:15, 23.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 76/100개 완료 | 이번에 걸린 시간: 15.6s | 예상 남은 시간: 13.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  77%|███████▋  | 77/100 [43:58<08:08, 21.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 77/100개 완료 | 이번에 걸린 시간: 16.7s | 예상 남은 시간: 13.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  78%|███████▊  | 78/100 [44:15<07:18, 19.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 78/100개 완료 | 이번에 걸린 시간: 16.9s | 예상 남은 시간: 12.5분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  79%|███████▉  | 79/100 [44:33<06:45, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 79/100개 완료 | 이번에 걸린 시간: 17.9s | 예상 남은 시간: 11.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  80%|████████  | 80/100 [44:49<06:07, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 80/100개 완료 | 이번에 걸린 시간: 16.2s | 예상 남은 시간: 11.2분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  81%|████████  | 81/100 [45:13<06:22, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 81/100개 완료 | 이번에 걸린 시간: 24.3s | 예상 남은 시간: 10.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  82%|████████▏ | 82/100 [45:43<06:57, 23.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 82/100개 완료 | 이번에 걸린 시간: 30.2s | 예상 남은 시간: 10.0분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  83%|████████▎ | 83/100 [46:11<06:58, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 83/100개 완료 | 이번에 걸린 시간: 28.0s | 예상 남은 시간: 9.5분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  84%|████████▍ | 84/100 [46:34<06:27, 24.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 84/100개 완료 | 이번에 걸린 시간: 23.2s | 예상 남은 시간: 8.9분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  85%|████████▌ | 85/100 [47:00<06:07, 24.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 85/100개 완료 | 이번에 걸린 시간: 25.3s | 예상 남은 시간: 8.3분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  86%|████████▌ | 86/100 [47:25<05:44, 24.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 86/100개 완료 | 이번에 걸린 시간: 24.8s | 예상 남은 시간: 7.7분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  87%|████████▋ | 87/100 [47:48<05:16, 24.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 87/100개 완료 | 이번에 걸린 시간: 23.7s | 예상 남은 시간: 7.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  88%|████████▊ | 88/100 [48:10<04:43, 23.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 88/100개 완료 | 이번에 걸린 시간: 21.8s | 예상 남은 시간: 6.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  89%|████████▉ | 89/100 [48:35<04:25, 24.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 89/100개 완료 | 이번에 걸린 시간: 25.4s | 예상 남은 시간: 6.0분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  90%|█████████ | 90/100 [48:56<03:49, 22.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 90/100개 완료 | 이번에 걸린 시간: 20.2s | 예상 남은 시간: 5.4분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  91%|█████████ | 91/100 [49:26<03:45, 25.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 91/100개 완료 | 이번에 걸린 시간: 30.1s | 예상 남은 시간: 4.9분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  92%|█████████▏| 92/100 [49:54<03:27, 25.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 92/100개 완료 | 이번에 걸린 시간: 28.0s | 예상 남은 시간: 4.3분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  93%|█████████▎| 93/100 [50:18<02:57, 25.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 93/100개 완료 | 이번에 걸린 시간: 24.0s | 예상 남은 시간: 3.8분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  94%|█████████▍| 94/100 [50:45<02:35, 25.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 94/100개 완료 | 이번에 걸린 시간: 27.4s | 예상 남은 시간: 3.2분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  95%|█████████▌| 95/100 [51:13<02:13, 26.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 95/100개 완료 | 이번에 걸린 시간: 28.2s | 예상 남은 시간: 2.7분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  96%|█████████▌| 96/100 [51:34<01:39, 24.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 96/100개 완료 | 이번에 걸린 시간: 21.1s | 예상 남은 시간: 2.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  97%|█████████▋| 97/100 [51:59<01:14, 24.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 97/100개 완료 | 이번에 걸린 시간: 24.2s | 예상 남은 시간: 1.6분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  98%|█████████▊| 98/100 [52:20<00:47, 23.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 98/100개 완료 | 이번에 걸린 시간: 21.5s | 예상 남은 시간: 1.1분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중:  99%|█████████▉| 99/100 [52:44<00:23, 23.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 99/100개 완료 | 이번에 걸린 시간: 24.4s | 예상 남은 시간: 0.5분\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search 진행 중: 100%|██████████| 100/100 [53:11<00:00, 31.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ 현재 100/100개 완료 | 이번에 걸린 시간: 26.9s | 예상 남은 시간: 0.0분\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ✅ 그리드 조합 준비\n",
    "from itertools import product\n",
    "combinations = list(product(weight_list, alpha_list))\n",
    "total = len(combinations)\n",
    "\n",
    "# ✅ tqdm 진행률 표시\n",
    "for idx, (class_weight_1, alpha) in enumerate(tqdm(combinations, desc=\"Grid Search 진행 중\")):\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    # 시작 시간 체크\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 🎯 모델 구성\n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    x = LSTM(64, return_sequences=True)(inputs)\n",
    "    x = LSTM(32)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "              validation_data=(X_test, y_test),\n",
    "              epochs=100,\n",
    "              batch_size=32,\n",
    "              callbacks=[early_stop],\n",
    "              verbose=0,\n",
    "              class_weight=class_weights)\n",
    "\n",
    "    # 🎯 예측 및 threshold별 반복\n",
    "    y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "    for threshold in threshold_list:\n",
    "        y_pred = (y_proba > threshold).astype(int)\n",
    "        macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        final_results.append({\n",
    "            'alpha': alpha,\n",
    "            'threshold': threshold,\n",
    "            'class_weight_1': class_weight_1,\n",
    "            'macro_f1': macro_f1\n",
    "        })\n",
    "\n",
    "    # ⏱️ ETA 계산 및 출력\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_time_per_iter = elapsed if idx == 0 else (time.time() - first_time) / (idx + 1)\n",
    "    remaining = avg_time_per_iter * (total - idx - 1)\n",
    "\n",
    "    print(f\"⏱️ 현재 {idx+1}/{total}개 완료 | 이번에 걸린 시간: {elapsed:.1f}s | 예상 남은 시간: {remaining/60:.1f}분\")\n",
    "\n",
    "    if idx == 0:\n",
    "        first_time = start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7b7b8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>threshold</th>\n",
       "      <th>class_weight_1</th>\n",
       "      <th>macro_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.686125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.778737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.572726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.458955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.448795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.620364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.723532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.775227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.696279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.458308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alpha  threshold  class_weight_1  macro_f1\n",
       "0     0.01        0.1             6.0  0.686125\n",
       "1     0.01        0.2             6.0  0.778737\n",
       "2     0.01        0.3             6.0  0.572726\n",
       "3     0.01        0.4             6.0  0.458955\n",
       "4     0.01        0.5             6.0  0.448795\n",
       "..     ...        ...             ...       ...\n",
       "895   0.91        0.5            15.0  0.620364\n",
       "896   0.91        0.6            15.0  0.723532\n",
       "897   0.91        0.7            15.0  0.775227\n",
       "898   0.91        0.8            15.0  0.696279\n",
       "899   0.91        0.9            15.0  0.458308\n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "807b4b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 그리드서치 성능 상위 5개 조합:\n",
      "     alpha  threshold  class_weight_1  macro_f1\n",
      "471   0.21        0.4            11.0  0.795047\n",
      "68    0.71        0.6             6.0  0.794613\n",
      "958   0.61        0.5             6.0  0.793220\n",
      "239   0.61        0.6             8.0  0.788727\n",
      "111   0.21        0.4             7.0  0.786582\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 그리드서치 결과 저장된 리스트 → DataFrame\n",
    "df_results = pd.DataFrame(final_results)\n",
    "\n",
    "# macro_f1 기준으로 상위 5개 추출\n",
    "top5 = df_results.sort_values(by='macro_f1', ascending=False).head(5)\n",
    "\n",
    "# 출력\n",
    "print(\"📊 그리드서치 성능 상위 5개 조합:\")\n",
    "print(top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b06e09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: -0.7, alpha: 0.21001, threshold: 0.4000, class_weight_1: 11.0001, macro f1-score: 0.8012\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.77      0.68        94\n",
      "           1       0.95      0.90      0.92       455\n",
      "\n",
      "    accuracy                           0.88       549\n",
      "   macro avg       0.78      0.83      0.80       549\n",
      "weighted avg       0.89      0.88      0.88       549\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[ 72  22]\n",
      " [ 46 409]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = -0.7\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "# ✅ 수동으로 최적 조합 지정\n",
    "best_alpha = 0.21\n",
    "best_threshold = 0.4\n",
    "best_weight = 11.0\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = LSTM(64, return_sequences=True)(inputs)\n",
    "x = LSTM(32)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b622c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0, alpha: 0.21001, threshold: 0.4000, class_weight_1: 11.0001, macro f1-score: 0.8125\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.79       252\n",
      "           1       0.82      0.84      0.83       297\n",
      "\n",
      "    accuracy                           0.81       549\n",
      "   macro avg       0.81      0.81      0.81       549\n",
      "weighted avg       0.81      0.81      0.81       549\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[197  55]\n",
      " [ 47 250]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = LSTM(64, return_sequences=True)(inputs)\n",
    "x = LSTM(32)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cda0f57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0, alpha: 0.71, threshold: 0.6000, class_weight_1: 11.0, macro f1-score: 0.7747\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.75       252\n",
      "           1       0.78      0.82      0.80       296\n",
      "\n",
      "    accuracy                           0.78       548\n",
      "   macro avg       0.78      0.77      0.77       548\n",
      "weighted avg       0.78      0.78      0.78       548\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[183  69]\n",
      " [ 53 243]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "\n",
    "change_cut = 0\n",
    "alpha_list = np.round(np.arange(0.01, 1.0, 0.1), 2)       # 성능 중심 구간\n",
    "threshold_list = np.round(np.arange(0.1, 1.0, 0.1), 2)    # 유효 범위 압축\n",
    "weight_list = np.round(np.arange(6.0, 16.0, 1.0), 2)      # class_weight_1\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = LSTM(64, return_sequences=True)(inputs)\n",
    "x = LSTM(32)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "772c9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 3-class target 생성\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change_pct'] = 100 * (df['next_day_close'] - df['중국환율_종가']) / df['중국환율_종가']\n",
    "\n",
    "def label_3class(x):\n",
    "    if x > 0.1:\n",
    "        return 1    # 상승\n",
    "    elif x < -0.1:\n",
    "        return -1   # 하락\n",
    "    else:\n",
    "        return 0    # 보합\n",
    "\n",
    "df['target'] = df['change_pct'].apply(label_3class)\n",
    "df = df.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c7a598df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change_pct'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8fae9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.4283 - loss: 1.0527 - val_accuracy: 0.5128 - val_loss: 1.0321\n",
      "Epoch 2/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5286 - loss: 0.9945 - val_accuracy: 0.5547 - val_loss: 0.9956\n",
      "Epoch 3/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5825 - loss: 0.9526 - val_accuracy: 0.6040 - val_loss: 0.9371\n",
      "Epoch 4/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6218 - loss: 0.8802 - val_accuracy: 0.6241 - val_loss: 0.8832\n",
      "Epoch 5/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6485 - loss: 0.8343 - val_accuracy: 0.6387 - val_loss: 0.8520\n",
      "Epoch 6/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6578 - loss: 0.8114 - val_accuracy: 0.6423 - val_loss: 0.8340\n",
      "Epoch 7/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6765 - loss: 0.7747 - val_accuracy: 0.6350 - val_loss: 0.8336\n",
      "Epoch 8/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6933 - loss: 0.7495 - val_accuracy: 0.6478 - val_loss: 0.8200\n",
      "Epoch 9/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7040 - loss: 0.7301 - val_accuracy: 0.6624 - val_loss: 0.7931\n",
      "Epoch 10/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7108 - loss: 0.7117 - val_accuracy: 0.6606 - val_loss: 0.7962\n",
      "Epoch 11/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7262 - loss: 0.6853 - val_accuracy: 0.6734 - val_loss: 0.7762\n",
      "Epoch 12/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7364 - loss: 0.6686 - val_accuracy: 0.6825 - val_loss: 0.7642\n",
      "Epoch 13/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7438 - loss: 0.6522 - val_accuracy: 0.6843 - val_loss: 0.7674\n",
      "Epoch 14/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7420 - loss: 0.6502 - val_accuracy: 0.6807 - val_loss: 0.7552\n",
      "Epoch 15/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7447 - loss: 0.6401 - val_accuracy: 0.6752 - val_loss: 0.7630\n",
      "Epoch 16/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7571 - loss: 0.6334 - val_accuracy: 0.6770 - val_loss: 0.7676\n",
      "Epoch 17/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7514 - loss: 0.6316 - val_accuracy: 0.6770 - val_loss: 0.7732\n",
      "Epoch 18/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7656 - loss: 0.6111 - val_accuracy: 0.6807 - val_loss: 0.7935\n",
      "Epoch 19/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7684 - loss: 0.6136 - val_accuracy: 0.6734 - val_loss: 0.7777\n",
      "Epoch 20/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7554 - loss: 0.6201 - val_accuracy: 0.6861 - val_loss: 0.7676\n",
      "Epoch 21/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7725 - loss: 0.6025 - val_accuracy: 0.6770 - val_loss: 0.7768\n",
      "Epoch 22/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7738 - loss: 0.5904 - val_accuracy: 0.6788 - val_loss: 0.7894\n",
      "Epoch 23/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7758 - loss: 0.5719 - val_accuracy: 0.6606 - val_loss: 0.8064\n",
      "Epoch 24/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.7771 - loss: 0.5617 - val_accuracy: 0.6515 - val_loss: 0.8122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15f4ec00fe0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# one-hot encoding\n",
    "y_train_cat = to_categorical(y_train + 1, num_classes=3)  # -1,0,1 → 0,1,2\n",
    "y_test_cat = to_categorical(y_test + 1, num_classes=3)\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = LSTM(64, return_sequences=True)(inputs)\n",
    "x = LSTM(32)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train_cat,\n",
    "          validation_data=(X_test, y_test_cat),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b1c6125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "📊 [3-Class 분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.51      0.55        45\n",
      "           0       0.89      0.95      0.92       456\n",
      "           1       0.68      0.36      0.47        47\n",
      "\n",
      "    accuracy                           0.86       548\n",
      "   macro avg       0.73      0.61      0.65       548\n",
      "weighted avg       0.85      0.86      0.85       548\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[ 23  21   1]\n",
      " [ 15 434   7]\n",
      " [  0  30  17]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_proba, axis=1) - 1  # 0,1,2 → -1,0,1\n",
    "\n",
    "print(\"\\n📊 [3-Class 분류 리포트]\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred, labels=[-1, 0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d2dc1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "📊 [3-Class +-0.2 기준분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.85      0.72       167\n",
      "           0       0.62      0.26      0.37       198\n",
      "           1       0.63      0.80      0.70       183\n",
      "\n",
      "    accuracy                           0.62       548\n",
      "   macro avg       0.62      0.64      0.60       548\n",
      "weighted avg       0.62      0.62      0.59       548\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[142  13  12]\n",
      " [ 70  52  76]\n",
      " [ 17  19 147]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_proba, axis=1) - 1  # 0,1,2 → -1,0,1\n",
    "\n",
    "print(\"\\n📊 [3-Class +-0.2 기준분류 리포트]\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred, labels=[-1, 0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2aca5f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "📊 [3-Class +-0.3 기준분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.58      0.75      0.65       127\n",
      "           0       0.70      0.69      0.69       277\n",
      "           1       0.77      0.58      0.66       144\n",
      "\n",
      "    accuracy                           0.68       548\n",
      "   macro avg       0.68      0.67      0.67       548\n",
      "weighted avg       0.69      0.68      0.68       548\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[ 95  29   3]\n",
      " [ 64 191  22]\n",
      " [  6  54  84]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_proba, axis=1) - 1  # 0,1,2 → -1,0,1\n",
    "\n",
    "print(\"\\n📊 [3-Class +-0.3 기준분류 리포트]\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred, labels=[-1, 0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9a5865cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "📊 [3-Class +-0.1 기준분류 리포트]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.68      0.87      0.77       202\n",
      "           0       0.54      0.06      0.11       118\n",
      "           1       0.68      0.83      0.75       228\n",
      "\n",
      "    accuracy                           0.68       548\n",
      "   macro avg       0.64      0.59      0.54       548\n",
      "weighted avg       0.65      0.68      0.62       548\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[176   3  23]\n",
      " [ 46   7  65]\n",
      " [ 35   3 190]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_proba, axis=1) - 1  # 0,1,2 → -1,0,1\n",
    "\n",
    "print(\"\\n📊 [3-Class +-0.1 기준분류 리포트]\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred, labels=[-1, 0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d564c9",
   "metadata": {},
   "source": [
    "# 보합 따로 후처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d49ae83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.5071 - loss: 0.7007 - val_accuracy: 0.5101 - val_loss: 0.6864\n",
      "Epoch 2/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5751 - loss: 0.6789 - val_accuracy: 0.4973 - val_loss: 0.6906\n",
      "Epoch 3/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5821 - loss: 0.6772 - val_accuracy: 0.5247 - val_loss: 0.6878\n",
      "Epoch 4/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5895 - loss: 0.6684 - val_accuracy: 0.5521 - val_loss: 0.6770\n",
      "Epoch 5/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5862 - loss: 0.6691 - val_accuracy: 0.5631 - val_loss: 0.6789\n",
      "Epoch 6/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5995 - loss: 0.6648 - val_accuracy: 0.5814 - val_loss: 0.6729\n",
      "Epoch 7/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6136 - loss: 0.6642 - val_accuracy: 0.5923 - val_loss: 0.6642\n",
      "Epoch 8/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6019 - loss: 0.6620 - val_accuracy: 0.6015 - val_loss: 0.6665\n",
      "Epoch 9/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6091 - loss: 0.6582 - val_accuracy: 0.6015 - val_loss: 0.6649\n",
      "Epoch 10/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6085 - loss: 0.6540 - val_accuracy: 0.5978 - val_loss: 0.6668\n",
      "Epoch 11/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6187 - loss: 0.6539 - val_accuracy: 0.6143 - val_loss: 0.6628\n",
      "Epoch 12/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6281 - loss: 0.6448 - val_accuracy: 0.6124 - val_loss: 0.6599\n",
      "Epoch 13/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6367 - loss: 0.6393 - val_accuracy: 0.6124 - val_loss: 0.6594\n",
      "Epoch 14/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6347 - loss: 0.6322 - val_accuracy: 0.5978 - val_loss: 0.6572\n",
      "Epoch 15/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6417 - loss: 0.6283 - val_accuracy: 0.5941 - val_loss: 0.6573\n",
      "Epoch 16/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6533 - loss: 0.6212 - val_accuracy: 0.6015 - val_loss: 0.6582\n",
      "Epoch 17/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6651 - loss: 0.6075 - val_accuracy: 0.6197 - val_loss: 0.6393\n",
      "Epoch 18/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6594 - loss: 0.6050 - val_accuracy: 0.5923 - val_loss: 0.6544\n",
      "Epoch 19/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6787 - loss: 0.5909 - val_accuracy: 0.6051 - val_loss: 0.6466\n",
      "Epoch 20/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6819 - loss: 0.5763 - val_accuracy: 0.5960 - val_loss: 0.6542\n",
      "Epoch 21/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7016 - loss: 0.5706 - val_accuracy: 0.6124 - val_loss: 0.6545\n",
      "Epoch 22/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6811 - loss: 0.5723 - val_accuracy: 0.6271 - val_loss: 0.6359\n",
      "Epoch 23/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7035 - loss: 0.5633 - val_accuracy: 0.6289 - val_loss: 0.6513\n",
      "Epoch 24/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7156 - loss: 0.5495 - val_accuracy: 0.6453 - val_loss: 0.6361\n",
      "Epoch 25/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7184 - loss: 0.5427 - val_accuracy: 0.6581 - val_loss: 0.6360\n",
      "Epoch 26/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7276 - loss: 0.5337 - val_accuracy: 0.6380 - val_loss: 0.6457\n",
      "Epoch 27/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7310 - loss: 0.5252 - val_accuracy: 0.6581 - val_loss: 0.6461\n",
      "Epoch 28/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7383 - loss: 0.5266 - val_accuracy: 0.6600 - val_loss: 0.6487\n",
      "Epoch 29/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7425 - loss: 0.5261 - val_accuracy: 0.6508 - val_loss: 0.6630\n",
      "Epoch 30/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7494 - loss: 0.5131 - val_accuracy: 0.6600 - val_loss: 0.6632\n",
      "Epoch 31/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7461 - loss: 0.5096 - val_accuracy: 0.6563 - val_loss: 0.6679\n",
      "Epoch 32/100\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7625 - loss: 0.4872 - val_accuracy: 0.6545 - val_loss: 0.6828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15f3beffbc0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🎯 1단계 target: 보합 여부\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change_pct'] = 100 * (df['next_day_close'] - df['중국환율_종가']) / df['중국환율_종가']\n",
    "df['target_stage1'] = (df['change_pct'].abs() <= 0.3).astype(int)  # 보합이면 1\n",
    "\n",
    "df1 = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df1.drop(columns=['Date', '기준년월', 'return', 'return_future', 'next_day_close', 'change_pct', 'target_stage1'], errors='ignore')\n",
    "y1 = df1['target_stage1'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq1 = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq1.append(y1[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq1 = np.array(y_seq1)\n",
    "\n",
    "X_train, X_test, y1_train, y1_test = train_test_split(X_seq, y_seq1, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 🧠 1단계 모델 (보합 vs 비보합)\n",
    "model1 = tf.keras.Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model1.fit(X_train, y1_train, validation_data=(X_test, y1_test), epochs=100, batch_size=32,\n",
    "           callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "799640ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5082 - loss: 0.6936 - val_accuracy: 0.5214 - val_loss: 0.6847\n",
      "Epoch 2/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5517 - loss: 0.6873 - val_accuracy: 0.5857 - val_loss: 0.6729\n",
      "Epoch 3/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6338 - loss: 0.6474 - val_accuracy: 0.6286 - val_loss: 0.6434\n",
      "Epoch 4/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6724 - loss: 0.6106 - val_accuracy: 0.6536 - val_loss: 0.6155\n",
      "Epoch 5/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7179 - loss: 0.5638 - val_accuracy: 0.6929 - val_loss: 0.5861\n",
      "Epoch 6/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7726 - loss: 0.5225 - val_accuracy: 0.7214 - val_loss: 0.5470\n",
      "Epoch 7/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7642 - loss: 0.5019 - val_accuracy: 0.7250 - val_loss: 0.5499\n",
      "Epoch 8/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7752 - loss: 0.4884 - val_accuracy: 0.7286 - val_loss: 0.5339\n",
      "Epoch 9/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7927 - loss: 0.4877 - val_accuracy: 0.7214 - val_loss: 0.5377\n",
      "Epoch 10/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7937 - loss: 0.4797 - val_accuracy: 0.7250 - val_loss: 0.5379\n",
      "Epoch 11/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7830 - loss: 0.4743 - val_accuracy: 0.7286 - val_loss: 0.5328\n",
      "Epoch 12/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8023 - loss: 0.4603 - val_accuracy: 0.7286 - val_loss: 0.5386\n",
      "Epoch 13/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7970 - loss: 0.4675 - val_accuracy: 0.7357 - val_loss: 0.5288\n",
      "Epoch 14/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8182 - loss: 0.4513 - val_accuracy: 0.7429 - val_loss: 0.5040\n",
      "Epoch 15/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8153 - loss: 0.4336 - val_accuracy: 0.7429 - val_loss: 0.5054\n",
      "Epoch 16/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8107 - loss: 0.4345 - val_accuracy: 0.7464 - val_loss: 0.5117\n",
      "Epoch 17/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8213 - loss: 0.4233 - val_accuracy: 0.7464 - val_loss: 0.5166\n",
      "Epoch 18/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8359 - loss: 0.4117 - val_accuracy: 0.7464 - val_loss: 0.5217\n",
      "Epoch 19/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8313 - loss: 0.4114 - val_accuracy: 0.7714 - val_loss: 0.5022\n",
      "Epoch 20/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8402 - loss: 0.3999 - val_accuracy: 0.7714 - val_loss: 0.5107\n",
      "Epoch 21/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8326 - loss: 0.4039 - val_accuracy: 0.7750 - val_loss: 0.4987\n",
      "Epoch 22/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8502 - loss: 0.3736 - val_accuracy: 0.7750 - val_loss: 0.5100\n",
      "Epoch 23/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8562 - loss: 0.3691 - val_accuracy: 0.7750 - val_loss: 0.5240\n",
      "Epoch 24/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8581 - loss: 0.3613 - val_accuracy: 0.7679 - val_loss: 0.5215\n",
      "Epoch 25/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8532 - loss: 0.3624 - val_accuracy: 0.7714 - val_loss: 0.5213\n",
      "Epoch 26/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8623 - loss: 0.3426 - val_accuracy: 0.7607 - val_loss: 0.5496\n",
      "Epoch 27/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8512 - loss: 0.3334 - val_accuracy: 0.7643 - val_loss: 0.5405\n",
      "Epoch 28/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8570 - loss: 0.3261 - val_accuracy: 0.7357 - val_loss: 0.5922\n",
      "Epoch 29/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8550 - loss: 0.3171 - val_accuracy: 0.7464 - val_loss: 0.6039\n",
      "Epoch 30/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8739 - loss: 0.3183 - val_accuracy: 0.7679 - val_loss: 0.5639\n",
      "Epoch 31/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8505 - loss: 0.3195 - val_accuracy: 0.7679 - val_loss: 0.5419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15f4ec01df0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🎯 2단계 대상: 비보합 구간 중 상승/하락\n",
    "df2 = df1[df1['target_stage1'] == 0].copy()\n",
    "df2['target_stage2'] = (df2['change_pct'] > 0).astype(int)  # 상승 1, 하락 0\n",
    "\n",
    "X2 = df2.drop(columns=['Date', '기준년월', 'return', 'return_future', 'next_day_close', 'change_pct', 'target_stage1', 'target_stage2'], errors='ignore')\n",
    "y2 = df2['target_stage2'].values\n",
    "\n",
    "X_scaled2 = scaler.fit_transform(X2)\n",
    "X_seq2, y_seq2 = [], []\n",
    "for i in range(seq_length, len(X_scaled2)):\n",
    "    X_seq2.append(X_scaled2[i-seq_length:i])\n",
    "    y_seq2.append(y2[i])\n",
    "X_seq2 = np.array(X_seq2)\n",
    "y_seq2 = np.array(y_seq2)\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X_seq2, y_seq2, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 🧠 2단계 모델 (상승 vs 하락)\n",
    "model2 = tf.keras.Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X2_train.shape[1], X2_train.shape[2])),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model2.fit(X2_train, y2_train, validation_data=(X2_test, y2_test), epochs=100, batch_size=32,\n",
    "           callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "07f44631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n"
     ]
    }
   ],
   "source": [
    "# 1단계 예측 (보합 여부)\n",
    "y1_pred = (model1.predict(X_test) > 0.5).astype(int).flatten()\n",
    "\n",
    "# 2단계 예측 (비보합만 → 상승 or 하락)\n",
    "final_pred = []\n",
    "i2 = 0  # 2단계 인덱스\n",
    "\n",
    "for i, is_draw in enumerate(y1_pred):\n",
    "    if is_draw == 1:\n",
    "        final_pred.append(0)  # 보합\n",
    "    else:\n",
    "        y2_hat = (model2.predict(np.expand_dims(X_test[i], axis=0)) > 0.5).astype(int).item()\n",
    "        final_pred.append(1 if y2_hat == 1 else -1)\n",
    "\n",
    "y_true_final = []\n",
    "for i in range(len(y1_test)):\n",
    "    if y1_test[i] == 1:\n",
    "        y_true_final.append(0)  # 보합\n",
    "    else:\n",
    "        y_true_final.append(1 if y2_test[i2] == 1 else -1)\n",
    "        i2 += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ea9902b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.26      0.39      0.31       127\n",
      "           0       0.69      0.47      0.56       276\n",
      "           1       0.30      0.36      0.33       144\n",
      "\n",
      "    accuracy                           0.42       547\n",
      "   macro avg       0.42      0.41      0.40       547\n",
      "weighted avg       0.49      0.42      0.44       547\n",
      "\n",
      "[[ 49  28  50]\n",
      " [ 75 131  70]\n",
      " [ 61  31  52]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_true_final, final_pred, labels=[-1, 0, 1]))\n",
    "print(confusion_matrix(y_true_final, final_pred, labels=[-1, 0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a663ff8a",
   "metadata": {},
   "source": [
    "# 0기준 2진분류 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91467658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('위안화최종데이터.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03aee6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('위안화최종데이터.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8c26b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0, alpha: 0.21, threshold: 0.4000, class_weight_1: 11.0001, macro f1-score: 0.8083\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80       253\n",
      "           1       0.86      0.77      0.81       295\n",
      "\n",
      "    accuracy                           0.81       548\n",
      "   macro avg       0.81      0.81      0.81       548\n",
      "weighted avg       0.81      0.81      0.81       548\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[215  38]\n",
      " [ 67 228]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "x = LSTM(128, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(16)(x)  # 마지막 LSTM은 return_sequences=False\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eb175c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 피처 수: 198\n",
      "제거된 피처 수: 1\n",
      "남은 피처 수: 197\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "\n",
    "# ✅ 1. 제거할 컬럼 정의\n",
    "exclude_cols = ['target', 'change', 'next_day_close', 'return', 'return_future', '기준년월', 'Date']\n",
    "\n",
    "# ✅ 2. 피처 리스트 추출\n",
    "feature_list = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# ✅ 3. X, y 정의\n",
    "X = df[feature_list]\n",
    "y = df['target']\n",
    "\n",
    "# ✅ 4. 랜덤포레스트로 피처 중요도 학습\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# ✅ 5. 중요도 DataFrame 생성\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=True)\n",
    "\n",
    "# ✅ 6. 중요도 ≤ 0 인 피처 제거\n",
    "low_importance_features = importance_df[importance_df['importance'] <= 0]['feature'].tolist()\n",
    "X_filtered = X.drop(columns=low_importance_features)\n",
    "\n",
    "# ✅ 7. 출력\n",
    "print(f\"전체 피처 수: {len(X.columns)}\")\n",
    "print(f\"제거된 피처 수: {len(low_importance_features)}\")\n",
    "print(f\"남은 피처 수: {X_filtered.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6acaec1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alpha86']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_importance_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da107a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bcbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd74ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b8fa1153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0, alpha: 0.21, threshold: 0.4000, class_weight_1: 15.0, macro f1-score: 0.8006\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.85      0.80       251\n",
      "           1       0.86      0.76      0.81       296\n",
      "\n",
      "    accuracy                           0.80       547\n",
      "   macro avg       0.80      0.80      0.80       547\n",
      "weighted avg       0.81      0.80      0.80       547\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[213  38]\n",
      " [ 71 225]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0\n",
    "alpha_list = np.round(np.arange(0.01, 1.0, 0.1), 2)       # 성능 중심 구간\n",
    "threshold_list = np.round(np.arange(0.1, 1.0, 0.1), 2)    # 유효 범위 압축\n",
    "weight_list = np.round(np.arange(6.0, 16.0, 1.0), 2)      # class_weight_1\n",
    "\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "x = LSTM(128, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(16)(x)  # 마지막 LSTM은 return_sequences=False\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5c2def41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0, alpha: 0.21, threshold: 0.4000, class_weight_1: 11.0, macro f1-score: 0.7102\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.59      0.65       244\n",
      "           1       0.71      0.83      0.77       297\n",
      "\n",
      "    accuracy                           0.72       541\n",
      "   macro avg       0.73      0.71      0.71       541\n",
      "weighted avg       0.72      0.72      0.72       541\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[143 101]\n",
      " [ 50 247]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-30)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "x = LSTM(128, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(16)(x)  # 마지막 LSTM은 return_sequences=False\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f23169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8287026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('위안화최종데이터.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bdd0eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0, alpha: 0.21, threshold: 0.4000, class_weight_1: 11.0, macro f1-score: 0.7993\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.82      0.79       250\n",
      "           1       0.84      0.79      0.81       300\n",
      "\n",
      "    accuracy                           0.80       550\n",
      "   macro avg       0.80      0.80      0.80       550\n",
      "weighted avg       0.80      0.80      0.80       550\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[204  46]\n",
      " [ 64 236]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "x = LSTM(128, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(16)(x)  # 마지막 LSTM은 return_sequences=False\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a97aebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('위안화최종데이터.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a2f7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['기준년월'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "760cd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ❶ Flatten 시계열: 평균을 통해 시간 축 제거 → [samples, features]\n",
    "X_test_flat = X_test.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7b3ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class KerasWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, threshold=0.5):\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 이미 학습된 모델이므로 fit은 패스\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 예측 확률을 받아서 threshold로 분류\n",
    "        X_seq = np.repeat(X[:, np.newaxis, :], seq_length, axis=1)  # reshape to [batch, time, feature]\n",
    "        probs = self.model.predict(X_seq).flatten()\n",
    "        return (probs > self.threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ffb8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, threshold=0.5):\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.classes_ = np.array([0, 1])  # ★ 필수 추가\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # LSTM이므로 다시 3D로 변환\n",
    "        X_seq = np.repeat(X[:, np.newaxis, :], seq_length, axis=1)\n",
    "        probs = self.model.predict(X_seq, verbose=0).flatten()\n",
    "        return (probs > self.threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0326541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
      "🎯 Macro F1: 0.7872\n",
      "\n",
      "🎯 상위 중요 피처\n",
      "     feature  importance_mean  importance_std\n",
      "170  alpha65         0.010331        0.007613\n",
      "145  alpha35         0.009120        0.007420\n",
      "141  alpha30         0.009018        0.009158\n",
      "137  alpha25         0.008270        0.006214\n",
      "168  alpha62         0.006765        0.004790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 44032 (\\N{HANGUL SYLLABLE GA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 52509 (\\N{HANGUL SYLLABLE CONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 50529 (\\N{HANGUL SYLLABLE AEG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 50984 (\\N{HANGUL SYLLABLE YUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51452 (\\N{HANGUL SYLLABLE JU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 49885 (\\N{HANGUL SYLLABLE SIG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 53076 (\\N{HANGUL SYLLABLE KO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 49828 (\\N{HANGUL SYLLABLE SEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 54588 (\\N{HANGUL SYLLABLE PI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 44160 (\\N{HANGUL SYLLABLE GEOM}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 49353 (\\N{HANGUL SYLLABLE SAEG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 47049 (\\N{HANGUL SYLLABLE RYANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 50976 (\\N{HANGUL SYLLABLE YU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51473 (\\N{HANGUL SYLLABLE JUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 44397 (\\N{HANGUL SYLLABLE GUG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 54872 (\\N{HANGUL SYLLABLE HWAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 48320 (\\N{HANGUL SYLLABLE BYEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 44256 (\\N{HANGUL SYLLABLE GO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51068 (\\N{HANGUL SYLLABLE IL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 48376 (\\N{HANGUL SYLLABLE BON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 50644 (\\N{HANGUL SYLLABLE EN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51333 (\\N{HANGUL SYLLABLE JONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 45787 (\\N{HANGUL SYLLABLE NIS}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 52992 (\\N{HANGUL SYLLABLE KE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51200 (\\N{HANGUL SYLLABLE JEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 45796 (\\N{HANGUL SYLLABLE DA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 50864 (\\N{HANGUL SYLLABLE U}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51316 (\\N{HANGUL SYLLABLE JON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 54620 (\\N{HANGUL SYLLABLE HAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 50808 (\\N{HANGUL SYLLABLE OE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51064 (\\N{HANGUL SYLLABLE IN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 48372 (\\N{HANGUL SYLLABLE BO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 48120 (\\N{HANGUL SYLLABLE MI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 49548 (\\N{HANGUL SYLLABLE SO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 47932 (\\N{HANGUL SYLLABLE MUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51648 (\\N{HANGUL SYLLABLE JI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 45797 (\\N{HANGUL SYLLABLE DAG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51312 (\\N{HANGUL SYLLABLE JO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 50896 (\\N{HANGUL SYLLABLE WEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 47196 (\\N{HANGUL SYLLABLE RO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 44144 (\\N{HANGUL SYLLABLE GEO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 47000 (\\N{HANGUL SYLLABLE RAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 54633 (\\N{HANGUL SYLLABLE HAB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 44544 (\\N{HANGUL SYLLABLE GEUL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 48268 (\\N{HANGUL SYLLABLE BEOL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51456 (\\N{HANGUL SYLLABLE JUN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 49328 (\\N{HANGUL SYLLABLE SAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 50629 (\\N{HANGUL SYLLABLE EOB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 49373 (\\N{HANGUL SYLLABLE SAENG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 44552 (\\N{HANGUL SYLLABLE GEUM}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 47532 (\\N{HANGUL SYLLABLE RI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 47749 (\\N{HANGUL SYLLABLE MYEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 47785 (\\N{HANGUL SYLLABLE MOG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 49900 (\\N{HANGUL SYLLABLE SIM}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 49901 (\\N{HANGUL SYLLABLE SIB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 50613 (\\N{HANGUL SYLLABLE EOG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 45804 (\\N{HANGUL SYLLABLE DAL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 47084 (\\N{HANGUL SYLLABLE REO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 52293 (\\N{HANGUL SYLLABLE CAEG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 52404 (\\N{HANGUL SYLLABLE CE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 52292 (\\N{HANGUL SYLLABLE CAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\dlsgh\\AppData\\Local\\Temp\\ipykernel_12756\\2819848047.py:136: UserWarning: Glyph 45380 (\\N{HANGUL SYLLABLE NYEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44032 (\\N{HANGUL SYLLABLE GA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 52509 (\\N{HANGUL SYLLABLE CONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50529 (\\N{HANGUL SYLLABLE AEG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50984 (\\N{HANGUL SYLLABLE YUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51452 (\\N{HANGUL SYLLABLE JU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49885 (\\N{HANGUL SYLLABLE SIG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 53076 (\\N{HANGUL SYLLABLE KO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49828 (\\N{HANGUL SYLLABLE SEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54588 (\\N{HANGUL SYLLABLE PI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44160 (\\N{HANGUL SYLLABLE GEOM}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49353 (\\N{HANGUL SYLLABLE SAEG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47049 (\\N{HANGUL SYLLABLE RYANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50976 (\\N{HANGUL SYLLABLE YU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51473 (\\N{HANGUL SYLLABLE JUNG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44397 (\\N{HANGUL SYLLABLE GUG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54872 (\\N{HANGUL SYLLABLE HWAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48320 (\\N{HANGUL SYLLABLE BYEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44256 (\\N{HANGUL SYLLABLE GO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51068 (\\N{HANGUL SYLLABLE IL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48376 (\\N{HANGUL SYLLABLE BON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50644 (\\N{HANGUL SYLLABLE EN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51333 (\\N{HANGUL SYLLABLE JONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45787 (\\N{HANGUL SYLLABLE NIS}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 52992 (\\N{HANGUL SYLLABLE KE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51200 (\\N{HANGUL SYLLABLE JEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45796 (\\N{HANGUL SYLLABLE DA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50864 (\\N{HANGUL SYLLABLE U}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51316 (\\N{HANGUL SYLLABLE JON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54620 (\\N{HANGUL SYLLABLE HAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50808 (\\N{HANGUL SYLLABLE OE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51064 (\\N{HANGUL SYLLABLE IN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48372 (\\N{HANGUL SYLLABLE BO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48120 (\\N{HANGUL SYLLABLE MI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49548 (\\N{HANGUL SYLLABLE SO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47932 (\\N{HANGUL SYLLABLE MUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51648 (\\N{HANGUL SYLLABLE JI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45797 (\\N{HANGUL SYLLABLE DAG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51312 (\\N{HANGUL SYLLABLE JO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50896 (\\N{HANGUL SYLLABLE WEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47196 (\\N{HANGUL SYLLABLE RO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44144 (\\N{HANGUL SYLLABLE GEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47000 (\\N{HANGUL SYLLABLE RAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54644 (\\N{HANGUL SYLLABLE HAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54633 (\\N{HANGUL SYLLABLE HAB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44544 (\\N{HANGUL SYLLABLE GEUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48268 (\\N{HANGUL SYLLABLE BEOL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51456 (\\N{HANGUL SYLLABLE JUN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49328 (\\N{HANGUL SYLLABLE SAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50629 (\\N{HANGUL SYLLABLE EOB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49373 (\\N{HANGUL SYLLABLE SAENG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44552 (\\N{HANGUL SYLLABLE GEUM}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47532 (\\N{HANGUL SYLLABLE RI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47749 (\\N{HANGUL SYLLABLE MYEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47785 (\\N{HANGUL SYLLABLE MOG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49900 (\\N{HANGUL SYLLABLE SIM}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49901 (\\N{HANGUL SYLLABLE SIB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50613 (\\N{HANGUL SYLLABLE EOG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45804 (\\N{HANGUL SYLLABLE DAL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47084 (\\N{HANGUL SYLLABLE REO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 52293 (\\N{HANGUL SYLLABLE CAEG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 52404 (\\N{HANGUL SYLLABLE CE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 52292 (\\N{HANGUL SYLLABLE CAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\dlsgh\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45380 (\\N{HANGUL SYLLABLE NYEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# ✅ 데이터 준비 (df는 이미 로드되어 있다고 가정)\n",
    "change_cut = 0\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ LSTM 모델 학습 (성능 고정용)\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = LSTM(128, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = LSTM(32, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = LSTM(16)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "lstm_model = Model(inputs, outputs)\n",
    "lstm_model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=0.21), metrics=['accuracy'])\n",
    "lstm_model.fit(X_train, y_train,\n",
    "               validation_data=(X_test, y_test),\n",
    "               epochs=100,\n",
    "               batch_size=32,\n",
    "               callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "               verbose=0,\n",
    "               class_weight={0: 1.0, 1: 11.0})\n",
    "\n",
    "# ✅ Threshold 기반 예측\n",
    "y_proba = lstm_model.predict(X_test).flatten()\n",
    "best_threshold = 0.4\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"🎯 Macro F1: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "\n",
    "# ✅ 시계열 평균 → 2D로 축소\n",
    "X_test_flat = X_test.mean(axis=1)\n",
    "\n",
    "# ✅ sklearn-style Wrapper 클래스\n",
    "class KerasWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, threshold=0.5):\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.classes_ = np.array([0, 1])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_seq = np.repeat(X[:, np.newaxis, :], seq_length, axis=1)\n",
    "        probs = self.model.predict(X_seq, verbose=0).flatten()\n",
    "        return (probs > self.threshold).astype(int)\n",
    "\n",
    "# ✅ Permutation Importance 실행\n",
    "wrapped_model = KerasWrapper(lstm_model, threshold=best_threshold)\n",
    "result = permutation_importance(\n",
    "    wrapped_model,\n",
    "    X_test_flat,\n",
    "    y_test,\n",
    "    scoring='f1_macro',\n",
    "    n_repeats=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ✅ 결과 정리\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance_mean': result.importances_mean,\n",
    "    'importance_std': result.importances_std\n",
    "}).sort_values(by='importance_mean', ascending=False)\n",
    "\n",
    "print(\"\\n🎯 상위 중요 피처\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# ✅ 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance_mean'], xerr=importance_df['importance_std'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Permutation Feature Importance (LSTM w/ mean over time)\")\n",
    "plt.xlabel(\"Decrease in Macro F1-score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e263d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature  importance_mean  importance_std\n",
      "170  alpha65         0.010331        0.007613\n",
      "145  alpha35         0.009120        0.007420\n",
      "141  alpha30         0.009018        0.009158\n",
      "137  alpha25         0.008270        0.006214\n",
      "168  alpha62         0.006765        0.004790\n",
      "121   alpha4         0.006266        0.008354\n",
      "129  alpha16         0.006235        0.004728\n",
      "167  alpha61         0.005960        0.002828\n",
      "143  alpha33         0.005582        0.005157\n",
      "126  alpha11         0.004848        0.001763\n",
      "115    sto_k         0.004774        0.005960\n",
      "140  alpha29         0.004610        0.002999\n",
      "180  alpha73         0.004607        0.005462\n",
      "123   alpha9         0.004398        0.008000\n",
      "103   std_60         0.004176        0.003303\n",
      "99    std_10         0.004068        0.003167\n",
      "111   rsi_14         0.004020        0.004248\n",
      "134  alpha21         0.003988        0.005841\n",
      "59   시가총액_비율         0.003822        0.001752\n",
      "91     lag_1         0.003795        0.004932\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(importance_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0752b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = ['alpha20',\n",
    " 'alpha64',\n",
    " 'alpha87',\n",
    " 'alpha43',\n",
    " '국고채(3년)',\n",
    " 'bb_upper',\n",
    " '일본EPU',\n",
    " '상해종합_종가',\n",
    " '한국정책금리',\n",
    " '한국(M1)조원',\n",
    " 'sto_d',\n",
    " 'sma_5',\n",
    " '코스피_시가',\n",
    " '나스닥_고가',\n",
    " 'alpha10',\n",
    " 'sma_20',\n",
    " '상해종합_시가',\n",
    " '금_거래량']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8cdd23d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0, alpha: 0.21, threshold: 0.4000, class_weight_1: 11.0, macro f1-score: 0.7982\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.88      0.80       250\n",
      "           1       0.88      0.73      0.80       300\n",
      "\n",
      "    accuracy                           0.80       550\n",
      "   macro avg       0.81      0.81      0.80       550\n",
      "weighted avg       0.81      0.80      0.80       550\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[221  29]\n",
      " [ 82 218]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "x = LSTM(128, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(16)(x)  # 마지막 LSTM은 return_sequences=False\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a7bfa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0, alpha: 0.21, threshold: 0.4000, class_weight_1: 11.0, macro f1-score: 0.7982\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.88      0.80       250\n",
      "           1       0.88      0.73      0.80       300\n",
      "\n",
      "    accuracy                           0.80       550\n",
      "   macro avg       0.81      0.81      0.80       550\n",
      "weighted avg       0.81      0.80      0.80       550\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[221  29]\n",
      " [ 82 218]]\n",
      "     feature  importance_mean  importance_std\n",
      "170  alpha65         0.010331        0.007613\n",
      "145  alpha35         0.009120        0.007420\n",
      "141  alpha30         0.009018        0.009158\n",
      "137  alpha25         0.008270        0.006214\n",
      "168  alpha62         0.006765        0.004790\n",
      "121   alpha4         0.006266        0.008354\n",
      "129  alpha16         0.006235        0.004728\n",
      "167  alpha61         0.005960        0.002828\n",
      "143  alpha33         0.005582        0.005157\n",
      "126  alpha11         0.004848        0.001763\n",
      "115    sto_k         0.004774        0.005960\n",
      "140  alpha29         0.004610        0.002999\n",
      "180  alpha73         0.004607        0.005462\n",
      "123   alpha9         0.004398        0.008000\n",
      "103   std_60         0.004176        0.003303\n",
      "99    std_10         0.004068        0.003167\n",
      "111   rsi_14         0.004020        0.004248\n",
      "134  alpha21         0.003988        0.005841\n",
      "59   시가총액_비율         0.003822        0.001752\n",
      "91     lag_1         0.003795        0.004932\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(importance_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "abd0107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0, alpha: 0.21, threshold: 0.4000, class_weight_1: 11.0001, macro f1-score: 0.8119\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       251\n",
      "           1       0.84      0.81      0.82       299\n",
      "\n",
      "    accuracy                           0.81       550\n",
      "   macro avg       0.81      0.81      0.81       550\n",
      "weighted avg       0.81      0.81      0.81       550\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[205  46]\n",
      " [ 57 242]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# 🎯 피처 숙청 반영\n",
    "X = df.drop(columns=[\n",
    "    'Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change', \n",
    "    *drop_features  # ← 중요도 낮은 애들 제거\n",
    "], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "x = LSTM(128, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(16)(x)  # 마지막 LSTM은 return_sequences=False\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1048a9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "컬럼 수: 180\n",
      "['중국환율_시가', '중국환율_고가', '중국환율_저가', '중국환율_종가', '중국환율_변동%', 'WTI유_종가', 'WTI유_시가', 'WTI유_고가', 'WTI유_저가', 'WTI유_거래량', 'WTI유_변동%', '금_종가', '금_시가', '금_고가', '금_저가', '금_거래량', '금_변동%', 'S&P500_종가', 'S&P500_시가', 'S&P500_저가', 'S&P500_변동%', '다우존스_종가', '다우존스_시가', '다우존스_고가', '다우존스_저가', '다우존스_거래량', '다우존스_변동%', '상해종합_종가', '상해종합_고가', '상해종합_저가', '상해종합_거래량', '상해종합_변동%', '닛케이_종가', '닛케이_시가', '닛케이_고가', '닛케이_저가', '닛케이_변동%', '코스피_종가', '코스피_시가', '코스피_고가', '코스피_저가', '코스피_거래량', '코스피_변동%', '나스닥_종가', '나스닥_시가', '나스닥_고가', '나스닥_저가', '나스닥_거래량', '나스닥_변동%', 'VIX_종가', 'VIX_시가', 'VIX_고가', 'VIX_저가', 'VIX_거래량', 'VIX_변동%', '시가총액_전체', '시가총액_외국인보유', '시가총액_비율', '주식수_전체', '주식수_외국인보유', '주식수_비율', 'CD금리(91일)', '국고채(3년)', '한국정책금리', '한국(M1)조원', '한국(M1)변동%', '한국(M2)변동%', '미국(M1)십억달러', '미국(M2)십억달러', '소비자심리지수', '생산자물가지수', '산업생산지수', '외환보유액(억달러)', '경상수지', '미국소비자물가지수', '비트코인_검색량', '일본엔_검색량', '유로(EUR)_검색량', 'S&P 500_검색량', '코스피_검색량', '일본EPU', '중국EPU', '한국EPU', '미국EPU', '글로벌EPU_명목GDP기준', '글로벌EPU_PPP기준', 'log_return', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'sma_5', 'std_5', 'sma_10', 'std_10', 'sma_20', 'std_20', 'sma_60', 'std_60', 'bb_upper', 'bb_lower', 'bb_width', 'volatility_20', 'momentum_5', 'momentum_10', 'momentum_20', 'rsi_14', 'macd', 'macd_sig', 'macd_hist', 'sto_k', 'sto_d', 'M2(100 million yuan)', 'Volume', 'M0(100 million yuan)', 'alpha2', 'alpha4', 'alpha7', 'alpha9', 'VWAP', 'alpha10', 'alpha11', 'alpha12', 'alpha13', 'alpha16', 'adv20', 'alpha17', 'alpha18', 'alpha20', 'alpha23', 'alpha24', 'alpha25', 'alpha28', 'alpha29', 'alpha30', 'alpha31', 'alpha33', 'alpha34', 'alpha35', 'alpha37', 'returns', 'alpha38', 'alpha41', 'alpha42', 'alpha43', 'alpha46', 'alpha47', 'alpha49', 'cap', 'alpha56', 'alpha57', 'alpha51', 'alpha54', 'alpha60', 'adv180', 'adv120', 'adv60', 'adv50', 'adv15', 'alpha67', 'alpha70', 'adv5', 'adv10', 'adv30', 'adv40', 'adv81', 'adv150', 'alpha73', 'alpha76', 'alpha77', 'alpha83', 'alpha84', 'alpha87', 'alpha90', 'alpha92', 'alpha93', 'alpha98', 'alpha100', 'alpha101']\n"
     ]
    }
   ],
   "source": [
    "print(f\"컬럼 수: {X.shape[1]}\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64da262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance_mean': model.feature_importances_\n",
    "}).sort_values(by='importance_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d1dbd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         feature  importance_mean\n",
      "189      alpha86         0.000000\n",
      "186      alpha81         0.000047\n",
      "195      alpha99         0.000061\n",
      "167      alpha61         0.000070\n",
      "170      alpha65         0.000084\n",
      "181      alpha74         0.000087\n",
      "138      alpha27         0.000089\n",
      "185      alpha79         0.000124\n",
      "168      alpha62         0.000156\n",
      "134      alpha21         0.000164\n",
      "172      alpha68         0.000167\n",
      "182      alpha75         0.000194\n",
      "66        미국정책금리         0.000276\n",
      "69      한국(M2)조원         0.000291\n",
      "169      alpha64         0.000348\n",
      "158      alpha58         0.000349\n",
      "29       상해종합_시가         0.000394\n",
      "19     S&P500_고가         0.000432\n",
      "73       소비자심리지수         0.000450\n",
      "78     미국소비자물가지수         0.000453\n",
      "79      비트코인_검색량         0.000468\n",
      "100       sma_20         0.000497\n",
      "63     CD금리(91일)         0.000498\n",
      "65        한국정책금리         0.000510\n",
      "130        adv20         0.000510\n",
      "83       코스피_검색량         0.000517\n",
      "81   유로(EUR)_검색량         0.000561\n",
      "155          cap         0.000571\n",
      "18     S&P500_시가         0.000595\n",
      "162       adv180         0.000597\n"
     ]
    }
   ],
   "source": [
    "least_important = importance_df.sort_values(by='importance_mean').head(30)\n",
    "print(least_important)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75da6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하위 18개 피처 추출\n",
    "least_important = importance_df.sort_values(by='importance_mean').head(18)\n",
    "drop_features = least_important['feature'].tolist()\n",
    "\n",
    "# 🎯 X에서 숙청 대상 제거\n",
    "X = df.drop(columns=[\n",
    "    'Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change', \n",
    "    *drop_features\n",
    "], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c4bbdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alpha86',\n",
       " 'alpha81',\n",
       " 'alpha99',\n",
       " 'alpha61',\n",
       " 'alpha65',\n",
       " 'alpha74',\n",
       " 'alpha27',\n",
       " 'alpha79',\n",
       " 'alpha62',\n",
       " 'alpha21',\n",
       " 'alpha68',\n",
       " 'alpha75',\n",
       " '미국정책금리',\n",
       " '한국(M2)조원',\n",
       " 'alpha64',\n",
       " 'alpha58',\n",
       " '상해종합_시가',\n",
       " 'S&P500_고가']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eefd961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0, alpha: 0.21, threshold: 0.4000, class_weight_1: 11.0001, macro f1-score: 0.8012\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.84      0.79       251\n",
      "           1       0.85      0.77      0.81       298\n",
      "\n",
      "    accuracy                           0.80       549\n",
      "   macro avg       0.80      0.80      0.80       549\n",
      "weighted avg       0.81      0.80      0.80       549\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[211  40]\n",
      " [ 69 229]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# 🎯 피처 숙청 반영\n",
    "X = df.drop(columns=[\n",
    "    'Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change', \n",
    "    *drop_features  # ← 중요도 낮은 애들 제거\n",
    "], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "x = LSTM(128, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(16)(x)  # 마지막 LSTM은 return_sequences=False\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c5271acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0, alpha: 0.21001, threshold: 0.4000, class_weight_1: 11.0, macro f1-score: 0.8139\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.81       252\n",
      "           1       0.85      0.79      0.82       297\n",
      "\n",
      "    accuracy                           0.81       549\n",
      "   macro avg       0.81      0.82      0.81       549\n",
      "weighted avg       0.82      0.81      0.81       549\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[212  40]\n",
      " [ 62 235]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = 0\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# 🎯 피처 숙청 반영\n",
    "X = df.drop(columns=[\n",
    "    'Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change', \n",
    "    *drop_features  # ← 중요도 낮은 애들 제거\n",
    "], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "from tensorflow.keras.layers import Bidirectional, Add, LayerNormalization, Attention\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "# 블록 1\n",
    "x1 = Bidirectional(LSTM(64, return_sequences=True))(inputs)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "\n",
    "# 블록 2 (Residual 연결)\n",
    "x2 = Bidirectional(LSTM(64, return_sequences=True))(x1)\n",
    "x2 = Dropout(0.3)(x2)\n",
    "x = Add()([x1, x2])  # Residual\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "# 블록 3 (Self-Attention)\n",
    "attention = Attention()([x, x])\n",
    "x = Add()([x, attention])  # Residual + Attention\n",
    "x = LSTM(32)(x)\n",
    "\n",
    "# 출력층\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8711eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: -0.5, alpha: 0.21, threshold: 0.4000, class_weight_1: 11.0001, macro f1-score: 0.7492\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.70      0.63       132\n",
      "           1       0.90      0.83      0.87       417\n",
      "\n",
      "    accuracy                           0.80       549\n",
      "   macro avg       0.74      0.77      0.75       549\n",
      "weighted avg       0.82      0.80      0.81       549\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[ 93  39]\n",
      " [ 69 348]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = -0.5\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# 🎯 피처 숙청 반영\n",
    "X = df.drop(columns=[\n",
    "    'Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change', \n",
    "    *drop_features  # ← 중요도 낮은 애들 제거\n",
    "], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "x = LSTM(128, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(16)(x)  # 마지막 LSTM은 return_sequences=False\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e729db4e",
   "metadata": {},
   "source": [
    "위 코드는 하락 범위를 좁게했음 (상승예측을 더 많이하도록)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "70b8b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2111bd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('model_up_cautious.h5')\n",
    "joblib.dump(scaler, 'scaler_up.pkl')\n",
    "\n",
    "best_threshold_up = 0.4000\n",
    "best_alpha_up = 0.21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e5b7d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\n",
      "✅ [최종 조합 적용 결과]\n",
      "change_cut: 0.5, alpha: 0.21001, threshold: 0.4000, class_weight_1: 11.0001, macro f1-score: 0.7518\n",
      "\n",
      "📊 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88       399\n",
      "           1       0.71      0.56      0.62       149\n",
      "\n",
      "    accuracy                           0.82       548\n",
      "   macro avg       0.78      0.74      0.75       548\n",
      "weighted avg       0.81      0.82      0.81       548\n",
      "\n",
      "🧩 혼동 행렬:\n",
      "[[365  34]\n",
      " [ 66  83]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# ✅ Seed 고정\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :흰색_확인_표시: 하이퍼파라미터 범위 설정\n",
    "\n",
    "change_cut = +0.5\n",
    "alpha_list = [0.21, 0.21001]       # 성능 중심 구간\n",
    "threshold_list = [0.4, 0.40001]    # 유효 범위 압축\n",
    "weight_list = [11.0, 11.0001]     # class_weight_1\n",
    "\n",
    "# ✅ 결과 저장\n",
    "final_results = []\n",
    "\n",
    "# 🎯 데이터 준비\n",
    "df = df\n",
    "df['next_day_close'] = df['중국환율_종가'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['중국환율_종가']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# 🎯 피처 숙청 반영\n",
    "X = df.drop(columns=[\n",
    "    'Date', '기준년월', 'return', 'return_future', 'target', 'next_day_close', 'change', \n",
    "    *drop_features  # ← 중요도 낮은 애들 제거\n",
    "], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 🎯 시퀀스 구성\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# 🎯 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# ✅ 튜닝 시작\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # 모델 구성\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# ✅ 최적 조합 선택\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# ✅ 최적 조합 재적용\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "x = LSTM(128, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = LSTM(16)(x)  # 마지막 LSTM은 return_sequences=False\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# ✅ 최종 예측 및 평가\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n✅ [최종 조합 적용 결과]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\n📊 분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"🧩 혼동 행렬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
