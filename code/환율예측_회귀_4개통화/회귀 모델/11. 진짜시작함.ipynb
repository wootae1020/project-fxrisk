{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd3f5eca-4824-4150-8cc9-335f42bd1a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­M1, í•œêµ­M2, ë¯¸êµ­M1, ë¯¸êµ­M2, ìƒì‚°ìë¬¼ê°€ì§€ìˆ˜, ì „ì‚°ì—…ìƒì‚°ì§€ìˆ˜, \n",
    "# ê²½ìƒìˆ˜ì§€, ë¯¸êµ­ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ ì»¬ëŸ¼ì€ 2025ë…„ 5ì›”ê¹Œì§€ì˜ ì»¬ëŸ¼ë§Œ ì¡´ì¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b70bc19-9b65-4730-9a99-f4a0fd56aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8578290-0200-4dc4-b1ef-eec9aea27c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ê¸°ì¤€ë…„ì›”   í•œêµ­(M1)ì¡°ì›  í•œêµ­(M1)ë³€ë™%\n",
      "660 2025-01-01  1277.5434       0.60\n",
      "661 2025-02-01  1282.0857       0.40\n",
      "662 2025-03-01  1279.5412      -0.20\n",
      "663 2025-04-01  1272.4983      -0.60\n",
      "664 2025-05-01  1261.9366      -0.83\n"
     ]
    }
   ],
   "source": [
    "df11 = pd.read_excel('í˜‘ì˜í†µí™”(M1).xlsx')\n",
    "df11.columns=['ê¸°ì¤€ë…„ì›”','í•œêµ­(M1)ì¡°ì›','í•œêµ­(M1)ë³€ë™%']\n",
    "df11['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df11['ê¸°ì¤€ë…„ì›”'], format='%Y-%m')\n",
    "\n",
    "prev_m1 = df11.loc[df11['ê¸°ì¤€ë…„ì›”'] == '2025-04-01', 'í•œêµ­(M1)ì¡°ì›'].values[0]\n",
    "\n",
    "change_pct = -0.83\n",
    "new_m1 = round(prev_m1 * (1 + change_pct / 100), 4)\n",
    "\n",
    "new_row = pd.DataFrame({\n",
    "    'ê¸°ì¤€ë…„ì›”': [pd.to_datetime('2025-05-01')],\n",
    "    'í•œêµ­(M1)ì¡°ì›': [new_m1],\n",
    "    'í•œêµ­(M1)ë³€ë™%': [change_pct]\n",
    "})\n",
    "\n",
    "df11 = pd.concat([df11, new_row], ignore_index=True)\n",
    "\n",
    "print(df11.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d81c80e4-9624-4d05-ba44-9f22dfd97c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ê¸°ì¤€ë…„ì›”   í•œêµ­(M2)ì¡°ì›  í•œêµ­(M2)ë³€ë™%\n",
      "468 2025-01-01  4203.8015       0.50\n",
      "469 2025-02-01  4231.5708       0.70\n",
      "470 2025-03-01  4227.6921      -0.10\n",
      "471 2025-04-01  4235.8187       0.20\n",
      "472 2025-05-01  4265.0458       0.69\n"
     ]
    }
   ],
   "source": [
    "df12 = pd.read_excel('ê´‘ì˜í†µí™”(M2).xlsx')\n",
    "df12.columns=['ê¸°ì¤€ë…„ì›”','í•œêµ­(M2)ì¡°ì›','í•œêµ­(M2)ë³€ë™%']\n",
    "df12['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df12['ê¸°ì¤€ë…„ì›”'], format='%Y-%m')\n",
    "\n",
    "prev_value = df12.loc[df12['ê¸°ì¤€ë…„ì›”'] == '2025-04-01', 'í•œêµ­(M2)ì¡°ì›'].values[0]\n",
    "\n",
    "change_pct = 0.69\n",
    "new_value = round(prev_value * (1 + change_pct / 100), 4)\n",
    "\n",
    "new_row = pd.DataFrame({\n",
    "    'ê¸°ì¤€ë…„ì›”': [pd.to_datetime('2025-05-01')],\n",
    "    'í•œêµ­(M2)ì¡°ì›': [new_value],\n",
    "    'í•œêµ­(M2)ë³€ë™%': [change_pct]\n",
    "})\n",
    "\n",
    "df12 = pd.concat([df12, new_row], ignore_index=True)\n",
    "\n",
    "print(df12.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39705b07-b101-4d09-afd9-35cdb6fdcd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ ë‹¤ì¤‘ íšŒê·€ ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼:\n",
      "2025-05-01 â–¶ ëª…ëª©GDPê¸°ì¤€: 466.11, PPPê¸°ì¤€: 452.09\n",
      "2025-06-01 â–¶ ëª…ëª©GDPê¸°ì¤€: 329.91, PPPê¸°ì¤€: 322.83\n"
     ]
    }
   ],
   "source": [
    "df23 = pd.read_csv('ê²½ì œì •ì±…ë¶ˆí™•ì‹¤ì„±ì§€ìˆ˜.csv')\n",
    "df23['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df23['ê¸°ì¤€ë…„ì›”'], format='%Y-%m')\n",
    "\n",
    "fill_values = [145.885, 194.465, 136.72, 124.96]\n",
    "nan_indices = df23[df23['í•œêµ­EPU'].isna()].index[-4:]\n",
    "for idx, value in zip(nan_indices, fill_values):\n",
    "    df23.loc[idx, 'í•œêµ­EPU'] = value\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "df23 = df23.sort_values('ê¸°ì¤€ë…„ì›”').reset_index(drop=True)\n",
    "\n",
    "feature_cols = ['ì¼ë³¸EPU', 'ì¤‘êµ­EPU', 'í•œêµ­EPU', 'ë¯¸êµ­EPU']\n",
    "target_cols = ['ê¸€ë¡œë²ŒEPU_ëª…ëª©GDPê¸°ì¤€', 'ê¸€ë¡œë²ŒEPU_PPPê¸°ì¤€']\n",
    "\n",
    "regression_forecast = {}\n",
    "\n",
    "for target in target_cols:\n",
    "    train_df = df23.loc[df23['ê¸°ì¤€ë…„ì›”'] <= '2025-04-01'].dropna(subset=feature_cols + [target])\n",
    "    \n",
    "    X_train = train_df[feature_cols].values\n",
    "    y_train = train_df[target].values\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred_df = df23[df23['ê¸°ì¤€ë…„ì›”'].isin(['2025-05-01', '2025-06-01'])].copy()\n",
    "    X_pred = pred_df[feature_cols].values\n",
    "\n",
    "    y_pred = model.predict(X_pred)\n",
    "    regression_forecast[target] = y_pred\n",
    "\n",
    "print(\"\\nğŸ“Œ ë‹¤ì¤‘ íšŒê·€ ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "for i, date in enumerate(['2025-05-01', '2025-06-01']):\n",
    "    print(f\"{date} â–¶ ëª…ëª©GDPê¸°ì¤€: {regression_forecast['ê¸€ë¡œë²ŒEPU_ëª…ëª©GDPê¸°ì¤€'][i]:.2f}, PPPê¸°ì¤€: {regression_forecast['ê¸€ë¡œë²ŒEPU_PPPê¸°ì¤€'][i]:.2f}\")\n",
    "\n",
    "idx_missing = df23[df23['ê¸€ë¡œë²ŒEPU_ëª…ëª©GDPê¸°ì¤€'].isna()].index\n",
    "\n",
    "df23.loc[idx_missing[0], 'ê¸€ë¡œë²ŒEPU_ëª…ëª©GDPê¸°ì¤€'] = regression_forecast['ê¸€ë¡œë²ŒEPU_ëª…ëª©GDPê¸°ì¤€'][0]\n",
    "df23.loc[idx_missing[1], 'ê¸€ë¡œë²ŒEPU_ëª…ëª©GDPê¸°ì¤€'] = regression_forecast['ê¸€ë¡œë²ŒEPU_ëª…ëª©GDPê¸°ì¤€'][1]\n",
    "df23.loc[idx_missing[0], 'ê¸€ë¡œë²ŒEPU_PPPê¸°ì¤€'] = regression_forecast['ê¸€ë¡œë²ŒEPU_PPPê¸°ì¤€'][0]\n",
    "df23.loc[idx_missing[1], 'ê¸€ë¡œë²ŒEPU_PPPê¸°ì¤€'] = regression_forecast['ê¸€ë¡œë²ŒEPU_PPPê¸°ì¤€'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c0842ba-d2a7-423b-ac16-69914d13d6ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# í™˜ìœ¨\n",
    "df = pd.read_csv('ë¯¸í™˜ìœ¨ë°ì´í„°.csv')\n",
    "df = df.drop('ê±°ë˜ëŸ‰', axis=1)\n",
    "df.columns = ['Date','ë¯¸í™˜ìœ¨_ì¢…ê°€','ë¯¸í™˜ìœ¨_ì‹œê°€','ë¯¸í™˜ìœ¨_ê³ ê°€','ë¯¸í™˜ìœ¨_ì €ê°€','ë¯¸í™˜ìœ¨_ë³€ë™%']\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# ì‹œì¥ë°ì´í„°\n",
    "df1 = pd.read_csv('WTIìœ  ì„ ë¬¼ ê³¼ê±° ë°ì´í„°.csv')\n",
    "df1.columns=['Date','WTIìœ _ì¢…ê°€','WTIìœ _ì‹œê°€','WTIìœ _ê³ ê°€','WTIìœ _ì €ê°€','WTIìœ _ê±°ë˜ëŸ‰','WTIìœ _ë³€ë™%']\n",
    "df1['Date'] = pd.to_datetime(df1['Date'])\n",
    "\n",
    "df2 = pd.read_csv('ê¸ˆ ì„ ë¬¼ ê³¼ê±° ë°ì´í„°.csv')\n",
    "df2.columns=['Date','ê¸ˆ_ì¢…ê°€','ê¸ˆ_ì‹œê°€','ê¸ˆ_ê³ ê°€','ê¸ˆ_ì €ê°€','ê¸ˆ_ê±°ë˜ëŸ‰','ê¸ˆ_ë³€ë™%']\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "\n",
    "df3 = pd.read_csv('S&P 500 ê³¼ê±° ë°ì´í„°.csv')\n",
    "df3 = df3.drop('ê±°ë˜ëŸ‰', axis=1)\n",
    "df3.columns=['Date','S&P500_ì¢…ê°€','S&P500_ì‹œê°€','S&P500_ê³ ê°€','S&P500_ì €ê°€','S&P500_ë³€ë™%']\n",
    "df3['Date'] = pd.to_datetime(df3['Date'])\n",
    "\n",
    "df4 = pd.read_csv('ë‹¤ìš°ì¡´ìŠ¤ ê³¼ê±° ë°ì´í„°.csv')\n",
    "df4.columns=['Date','ë‹¤ìš°ì¡´ìŠ¤_ì¢…ê°€','ë‹¤ìš°ì¡´ìŠ¤_ì‹œê°€','ë‹¤ìš°ì¡´ìŠ¤_ê³ ê°€','ë‹¤ìš°ì¡´ìŠ¤_ì €ê°€','ë‹¤ìš°ì¡´ìŠ¤_ê±°ë˜ëŸ‰','ë‹¤ìš°ì¡´ìŠ¤_ë³€ë™%']\n",
    "df4['Date'] = pd.to_datetime(df4['Date'])\n",
    "\n",
    "df5 = pd.read_csv('ìƒí•´ì¢…í•© ê³¼ê±° ë°ì´í„°.csv')\n",
    "df5.columns=['Date','ìƒí•´ì¢…í•©_ì¢…ê°€','ìƒí•´ì¢…í•©_ì‹œê°€','ìƒí•´ì¢…í•©_ê³ ê°€','ìƒí•´ì¢…í•©_ì €ê°€','ìƒí•´ì¢…í•©_ê±°ë˜ëŸ‰','ìƒí•´ì¢…í•©_ë³€ë™%']\n",
    "df5['Date'] = pd.to_datetime(df5['Date'])\n",
    "\n",
    "df6 = pd.read_csv('ë‹›ì¼€ì´ ê³¼ê±° ë°ì´í„°.csv')\n",
    "df6 = df6.drop('ê±°ë˜ëŸ‰', axis=1)\n",
    "df6.columns=['Date','ë‹›ì¼€ì´_ì¢…ê°€','ë‹›ì¼€ì´_ì‹œê°€','ë‹›ì¼€ì´_ê³ ê°€','ë‹›ì¼€ì´_ì €ê°€','ë‹›ì¼€ì´_ë³€ë™%']\n",
    "df6['Date'] = pd.to_datetime(df6['Date'])\n",
    "\n",
    "df7 = pd.read_csv('ì½”ìŠ¤í”¼ì§€ìˆ˜ ê³¼ê±° ë°ì´í„°.csv')\n",
    "df7.columns=['Date','ì½”ìŠ¤í”¼_ì¢…ê°€','ì½”ìŠ¤í”¼_ì‹œê°€','ì½”ìŠ¤í”¼_ê³ ê°€','ì½”ìŠ¤í”¼_ì €ê°€','ì½”ìŠ¤í”¼_ê±°ë˜ëŸ‰','ì½”ìŠ¤í”¼_ë³€ë™%']\n",
    "df7['Date'] = pd.to_datetime(df7['Date'])\n",
    "\n",
    "df8 = pd.read_csv('ë‚˜ìŠ¤ë‹¥ì¢…í•©ì§€ìˆ˜ ê³¼ê±° ë°ì´í„°.csv')\n",
    "df8.columns=['Date','ë‚˜ìŠ¤ë‹¥_ì¢…ê°€','ë‚˜ìŠ¤ë‹¥_ì‹œê°€','ë‚˜ìŠ¤ë‹¥_ê³ ê°€','ë‚˜ìŠ¤ë‹¥_ì €ê°€','ë‚˜ìŠ¤ë‹¥_ê±°ë˜ëŸ‰','ë‚˜ìŠ¤ë‹¥_ë³€ë™%']\n",
    "df8['Date'] = pd.to_datetime(df8['Date'])\n",
    "\n",
    "df9 = pd.read_csv('S&P 500 VIX ì„ ë¬¼ ê³¼ê±° ë°ì´í„°.csv')\n",
    "df9.columns=['Date','VIX_ì¢…ê°€','VIX_ì‹œê°€','VIX_ê³ ê°€','VIX_ì €ê°€','VIX_ê±°ë˜ëŸ‰','VIX_ë³€ë™%']\n",
    "df9['Date'] = pd.to_datetime(df9['Date'])\n",
    "\n",
    "df10 = pd.read_csv('êµ­ë‚´ì£¼ì‹ë§¤ìˆ˜ëŸ‰ê´€ë ¨ë°ì´í„°.csv', encoding='cp949')\n",
    "df10.columns=['Date','ì‹œê°€ì´ì•¡_ì „ì²´','ì‹œê°€ì´ì•¡_ì™¸êµ­ì¸ë³´ìœ ','ì‹œê°€ì´ì•¡_ë¹„ìœ¨','ì£¼ì‹ìˆ˜_ì „ì²´','ì£¼ì‹ìˆ˜_ì™¸êµ­ì¸ë³´ìœ ','ì£¼ì‹ìˆ˜_ë¹„ìœ¨']\n",
    "df10['Date'] = pd.to_datetime(df10['Date'])\n",
    "\n",
    "# ê±°ì‹œê²½ì œì§€í‘œ\n",
    "# 5\n",
    "# df11 = pd.read_excel('í˜‘ì˜í†µí™”(M1).xlsx')\n",
    "# df11.columns=['ê¸°ì¤€ë…„ì›”','í•œêµ­(M1)ì¡°ì›','í•œêµ­(M1)ë³€ë™%']\n",
    "# df11['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df11['ê¸°ì¤€ë…„ì›”'], format='%Y-%m')\n",
    "\n",
    "# 5\n",
    "# df12 = pd.read_excel('ê´‘ì˜í†µí™”(M2).xlsx')\n",
    "# df12.columns=['ê¸°ì¤€ë…„ì›”','í•œêµ­(M2)ì¡°ì›','í•œêµ­(M2)ë³€ë™%']\n",
    "# df12['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df12['ê¸°ì¤€ë…„ì›”'], format='%Y-%m')\n",
    "\n",
    "# 5\n",
    "df13 = pd.read_excel('ë¯¸êµ­ í†µí™”ì§€í‘œ.xlsx')\n",
    "df13.columns=['ê¸°ì¤€ë…„ì›”','ë¯¸êµ­(M1)ì‹­ì–µë‹¬ëŸ¬','ë¯¸êµ­(M2)ì‹­ì–µë‹¬ëŸ¬']\n",
    "df13['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df13['ê¸°ì¤€ë…„ì›”'], format='%Y-%m')\n",
    "\n",
    "df14 = pd.read_csv('ì†Œë¹„ìì§€ìˆ˜0901~2506.csv', encoding='cp949')\n",
    "df14.columns=['ê¸°ì¤€ë…„ì›”','ì†Œë¹„ìì‹¬ë¦¬ì§€ìˆ˜']\n",
    "df14['ê¸°ì¤€ë…„ì›”'] = df14['ê¸°ì¤€ë…„ì›”'].astype(str)\n",
    "df14['ê¸°ì¤€ë…„ì›”'] = df14['ê¸°ì¤€ë…„ì›”'].str.replace(r'(^\\d{4})\\.1$', r'\\1.10', regex=True)\n",
    "df14['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df14['ê¸°ì¤€ë…„ì›”'], format='%Y.%m')\n",
    "\n",
    "# 5\n",
    "df15 = pd.read_csv('ìƒì‚°ìë¬¼ê°€ì§€ìˆ˜0901~2505.csv', encoding='cp949')\n",
    "df15.columns=['ê¸°ì¤€ë…„ì›”','ìƒì‚°ìë¬¼ê°€ì§€ìˆ˜']\n",
    "df15['ê¸°ì¤€ë…„ì›”'] = df15['ê¸°ì¤€ë…„ì›”'].astype(str)\n",
    "df15['ê¸°ì¤€ë…„ì›”'] = df15['ê¸°ì¤€ë…„ì›”'].str.replace(r'(^\\d{4})\\.1$', r'\\1.10', regex=True)\n",
    "df15['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df15['ê¸°ì¤€ë…„ì›”'], format='%Y.%m')\n",
    "\n",
    "# 5\n",
    "df16 = pd.read_csv('ì „ì‚°ì—…ìƒì‚°ì§€ìˆ˜0901~2505.csv', encoding='cp949')\n",
    "df16.columns=['ê¸°ì¤€ë…„ì›”','ì‚°ì—…ìƒì‚°ì§€ìˆ˜']\n",
    "df16['ê¸°ì¤€ë…„ì›”'] = df16['ê¸°ì¤€ë…„ì›”'].astype(str)\n",
    "df16['ê¸°ì¤€ë…„ì›”'] = df16['ê¸°ì¤€ë…„ì›”'].str.replace(r'(^\\d{4})\\.1$', r'\\1.10', regex=True)\n",
    "df16['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df16['ê¸°ì¤€ë…„ì›”'], format='%Y.%m')\n",
    "\n",
    "df17 = pd.read_csv('ëŒ€í•œë¯¼êµ­ì™¸í™˜ë³´ìœ ì•¡0901~2506.csv')\n",
    "df17['ê¸°ì¤€ë…„ì›”'] = df17['ê¸°ì¤€ë…„ì›”'].astype(str)\n",
    "df17['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df17['ê¸°ì¤€ë…„ì›”'], format='%Y%m')\n",
    "\n",
    "df18 = pd.read_csv('CDê¸ˆë¦¬_êµ­ê³ ì±„0901~2507.csv')\n",
    "df18.columns=['Date','CDê¸ˆë¦¬(91ì¼)','êµ­ê³ ì±„(3ë…„)']\n",
    "df18['Date'] = pd.to_datetime(df18['Date'], format='%Y/%m/%d')\n",
    "\n",
    "df19 = pd.read_csv('í•œë¯¸ê¸°ì¤€ê¸ˆë¦¬.csv')\n",
    "df19.columns=['Date','í•œêµ­ì •ì±…ê¸ˆë¦¬','ë¯¸êµ­ì •ì±…ê¸ˆë¦¬']\n",
    "df19['Date'] = pd.to_datetime(df19['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# 5\n",
    "df20 = pd.read_csv('ê²½ìƒìˆ˜ì§€0901~2505.csv', encoding='cp949')\n",
    "df20['ê¸°ì¤€ë…„ì›”'] = df20['ê¸°ì¤€ë…„ì›”'].astype(str)\n",
    "df20['ê¸°ì¤€ë…„ì›”'] = df20['ê¸°ì¤€ë…„ì›”'].str.replace(r'(^\\d{4})\\.1$', r'\\1.10', regex=True)\n",
    "df20['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df20['ê¸°ì¤€ë…„ì›”'], format='%Y.%m')\n",
    "\n",
    "# 5\n",
    "df21 = pd.read_csv('ë¯¸êµ­CPI.csv')\n",
    "df21['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df21['ê¸°ì¤€ë…„ì›”'], format='%Y-%m')\n",
    "df21.columns=['ê¸°ì¤€ë…„ì›”','ë¯¸êµ­ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜']\n",
    "\n",
    "# ì›¹ë°ì´í„°\n",
    "df22 = pd.read_csv('êµ¬ê¸€ê²€ìƒ‰ëŸ‰.csv')\n",
    "df22['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df22['ê¸°ì¤€ë…„ì›”'], format='%Y-%m')\n",
    "\n",
    "# df23 = pd.read_csv('ê²½ì œì •ì±…ë¶ˆí™•ì‹¤ì„±ì§€ìˆ˜.csv')\n",
    "# df23['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df23['ê¸°ì¤€ë…„ì›”'], format='%Y-%m')\n",
    "\n",
    "# ì¸í˜¸ê°€ ë§Œë“  íŒŒìƒë³€ìˆ˜\n",
    "df24 = pd.read_csv('ê²°ì¸¡ì œê±°_í™˜ìœ¨íŒŒìƒë³€ìˆ˜.csv')\n",
    "df24['Date'] = pd.to_datetime(df24['Date'])\n",
    "bool_cols = df24.select_dtypes(include='bool').columns\n",
    "df24[bool_cols] = df24[bool_cols].astype(int)\n",
    "\n",
    "# âœ… ê¸°ì¤€ì´ ë˜ëŠ” í™˜ìœ¨ ë°ì´í„°\n",
    "df_base = df.copy()\n",
    "\n",
    "# âœ… 1. ì¼ë³„ ë°ì´í„° ë³‘í•© (df1 ~ df10) + (df18, df19)\n",
    "daily_dfs = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df18, df19, df24]\n",
    "for df_ in daily_dfs:\n",
    "    df_base = pd.merge(df_base, df_, on='Date', how='left')\n",
    "\n",
    "# âœ… 2. ì›”ë³„ ë°ì´í„° ë³‘í•©ì„ ìœ„í•œ 'ê¸°ì¤€ë…„ì›”' ì»¬ëŸ¼ ìƒì„±\n",
    "df_base['ê¸°ì¤€ë…„ì›”'] = df_base['Date'].dt.to_period('M').astype(str)\n",
    "df_base['ê¸°ì¤€ë…„ì›”'] = pd.to_datetime(df_base['ê¸°ì¤€ë…„ì›”'], format='%Y-%m')\n",
    "\n",
    "# âœ… 3. ì›”ë³„ ë°ì´í„° ë³‘í•© (df11 ~ df21)\n",
    "monthly_dfs = [df11, df12, df13, df14, df15, df16, df17, df20, df21]\n",
    "for df_ in monthly_dfs:\n",
    "    df_base = pd.merge(df_base, df_, on='ê¸°ì¤€ë…„ì›”', how='left')\n",
    "\n",
    "# âœ… 4. ì›¹ë°ì´í„° ë³‘í•© (df22, df23)\n",
    "df_base = pd.merge(df_base, df22, on='ê¸°ì¤€ë…„ì›”', how='left')\n",
    "df_base = pd.merge(df_base, df23, on='ê¸°ì¤€ë…„ì›”', how='left')\n",
    "\n",
    "# âœ… 5. í•„ìš” ì‹œ ê¸°ì¤€ë…„ì›” ì œê±°\n",
    "# df_base.drop('ê¸°ì¤€ë…„ì›”', axis=1, inplace=True)\n",
    "\n",
    "def convert_volume(val):\n",
    "    try:\n",
    "        val = str(val).replace(',', '').strip()\n",
    "        if val.endswith('M'):\n",
    "            return float(val[:-1]) * 1e6\n",
    "        elif val.endswith('B'):\n",
    "            return float(val[:-1]) * 1e9\n",
    "        elif val.endswith('K'):\n",
    "            return float(val[:-1]) * 1e3\n",
    "        else:\n",
    "            return float(val)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "volume_columns = [col for col in df_base.columns if 'ê±°ë˜ëŸ‰' in col]\n",
    "\n",
    "for col in volume_columns:\n",
    "    df_base[col] = df_base[col].apply(convert_volume)\n",
    "\n",
    "# 1. object íƒ€ì… ì»¬ëŸ¼ ì¤‘ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ê²ƒ ì„ íƒ\n",
    "obj_cols = df_base.select_dtypes(include='object').columns\n",
    "\n",
    "# 2. ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ ì‹œë„ (ì½¤ë§ˆ, % ì œê±° í¬í•¨)\n",
    "for col in obj_cols:\n",
    "    df_base[col] = (\n",
    "        df_base[col]\n",
    "        .astype(str)\n",
    "        .str.replace(',', '', regex=False)  # ì‰¼í‘œ ì œê±°\n",
    "        .str.replace('%', '', regex=False)  # % ê¸°í˜¸ ì œê±°\n",
    "        .replace('-', '')                  # ìŒìˆ˜ ì•„ë‹Œ '-' ë¹ˆê°’ ì²˜ë¦¬\n",
    "    )\n",
    "    # ìˆ«ìë¡œ ë³€í™˜ (ì—ëŸ¬ ì‹œ NaN)\n",
    "    df_base[col] = pd.to_numeric(df_base[col], errors='coerce')\n",
    "\n",
    "# ë¬´í•œëŒ€ ê²°ì¸¡ìœ¼ë¡œ ì¹˜í™˜\n",
    "df_base.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# 2025 6ì›”, 7ì›” ì œì™¸\n",
    "cutoff_date = pd.to_datetime('2025-05-31')\n",
    "df_base = df_base[df_base['Date'] <= cutoff_date].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d29eced-4c2c-4b62-a49a-9dfb83c39322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì œê±°í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\n",
    "cols_to_drop = ['Close', 'Open', 'High', 'Low', 'ë¯¸í™˜ìœ¨_ë³€ë™%_y']\n",
    "df_base = df_base.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# âœ… ì»¬ëŸ¼ëª… ë³€ê²½\n",
    "df_base = df_base.rename(columns={'ë¯¸í™˜ìœ¨_ë³€ë™%_x': 'ë¯¸í™˜ìœ¨_ë³€ë™%'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36fd5fa7-d3c5-4f61-8fba-98d1ce6192b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.to_csv('ì–´ì©Œë©´ìµœì¢…ë°ì´í„°.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a233353-e77e-4f71-8987-5fa3560c6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ê²°ì¸¡ê°’ ê°œìˆ˜ ë° ë¹„ìœ¨ ê³„ì‚°\n",
    "null_df = pd.DataFrame({\n",
    "    'ê²°ì¸¡ê°’ ê°œìˆ˜': df_base.isnull().sum(),\n",
    "    'ê²°ì¸¡ ë¹„ìœ¨ (%)': df_base.isnull().mean() * 100\n",
    "})\n",
    "\n",
    "# âœ… 'alpha'ê°€ ì»¬ëŸ¼ëª…ì— í¬í•¨ëœ ê²ƒ ì¤‘ì—ì„œë§Œ ê²°ì¸¡ ë¹„ìœ¨ 5% ì´ìƒì¸ ì»¬ëŸ¼ í•„í„°ë§\n",
    "alpha_cols = [col for col in df_base.columns if 'alpha' in col]\n",
    "alpha_null_df = null_df.loc[alpha_cols]\n",
    "drop_cols = alpha_null_df[alpha_null_df['ê²°ì¸¡ ë¹„ìœ¨ (%)'] >= 5].index.tolist()\n",
    "\n",
    "# âœ… í•´ë‹¹ alpha ì»¬ëŸ¼ë“¤ë§Œ ì‚­ì œ\n",
    "df_base = df_base.drop(columns=drop_cols)\n",
    "\n",
    "# âœ… ë‚¨ì€ ê²°ì¸¡ê°’ ì „ì²´ ì‚­ì œ (ë‹¨, alpha ì™¸ ì»¬ëŸ¼ í¬í•¨)\n",
    "df_base = df_base.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d79c0f79-22e5-4bd9-98ec-9b0173218323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2009-10-09\n",
       "1      2009-10-13\n",
       "2      2009-10-14\n",
       "3      2009-10-15\n",
       "4      2009-10-16\n",
       "          ...    \n",
       "2679   2025-05-21\n",
       "2680   2025-05-27\n",
       "2681   2025-05-28\n",
       "2682   2025-05-29\n",
       "2683   2025-05-30\n",
       "Name: Date, Length: 2684, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base = df_base.sort_values(by='Date').reset_index(drop=True)\n",
    "df_base.Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7f9d7-cf6a-4a5d-926f-a640ffbf4e6e",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d7a0097-57cf-41a1-8eed-d36392a235b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒìŠ¹ / í•˜ë½ë°ë³´í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6eca87f1-c14f-4948-bd5e-e71c2c697b57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "âœ… [ìµœì¢… ì¡°í•© ì ìš© ê²°ê³¼]\n",
      "change_cut: 5, alpha: 0.21, threshold: 0.3000, class_weight_1: 12.0, macro f1-score: 0.5224\n",
      "\n",
      "ğŸ“Š ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.67      0.71       390\n",
      "           1       0.30      0.39      0.34       145\n",
      "\n",
      "    accuracy                           0.59       535\n",
      "   macro avg       0.52      0.53      0.52       535\n",
      "weighted avg       0.63      0.59      0.61       535\n",
      "\n",
      "ğŸ§© í˜¼ë™ í–‰ë ¬:\n",
      "[[261 129]\n",
      " [ 89  56]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# âœ… Seed ê³ ì •\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# âœ… Focal Loss ì •ì˜\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •\n",
    "change_cut = 5\n",
    "alpha_list = np.round(np.arange(0.01, 1.0, 0.1), 2)       # ì„±ëŠ¥ ì¤‘ì‹¬ êµ¬ê°„\n",
    "threshold_list = np.round(np.arange(0.1, 1.0, 0.1), 2)    # ìœ íš¨ ë²”ìœ„ ì••ì¶•\n",
    "weight_list = np.round(np.arange(6.0, 16.0, 1.0), 2)      # class_weight_1\n",
    "\n",
    "# âœ… ê²°ê³¼ ì €ì¥\n",
    "final_results = []\n",
    "\n",
    "# ğŸ¯ ë°ì´í„° ì¤€ë¹„\n",
    "df = df_base.copy()\n",
    "df['next_day_close'] = df['ë¯¸í™˜ìœ¨_ì¢…ê°€'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['ë¯¸í™˜ìœ¨_ì¢…ê°€']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', 'ê¸°ì¤€ë…„ì›”', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ğŸ¯ ì‹œí€€ìŠ¤ êµ¬ì„±\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# ğŸ¯ train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# âœ… íŠœë‹ ì‹œì‘\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # ëª¨ë¸ êµ¬ì„±\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# âœ… ìµœì  ì¡°í•© ì„ íƒ\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# âœ… ìµœì  ì¡°í•© ì¬ì ìš©\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = LSTM(64, return_sequences=True)(inputs)\n",
    "x = LSTM(32)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# âœ… ìµœì¢… ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\nâœ… [ìµœì¢… ì¡°í•© ì ìš© ê²°ê³¼]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\nğŸ“Š ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ğŸ§© í˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b252831-1b5b-478d-83ca-1a82f8691f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ë½ / ìƒìŠ¹ë°ë³´í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8126c360-1ee1-4f2c-adb8-19e52f62ac36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "\n",
      "âœ… [ìµœì¢… ì¡°í•© ì ìš© ê²°ê³¼]\n",
      "change_cut: 5, alpha: 0.71, threshold: 0.5000, class_weight_1: 10.0, macro f1-score: 0.5560\n",
      "\n",
      "ğŸ“Š ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79       399\n",
      "           1       0.35      0.30      0.32       136\n",
      "\n",
      "    accuracy                           0.68       535\n",
      "   macro avg       0.56      0.55      0.56       535\n",
      "weighted avg       0.66      0.68      0.67       535\n",
      "\n",
      "ğŸ§© í˜¼ë™ í–‰ë ¬:\n",
      "[[322  77]\n",
      " [ 95  41]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# âœ… Seed ê³ ì •\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# âœ… Focal Loss ì •ì˜\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# :í°ìƒ‰_í™•ì¸_í‘œì‹œ: í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •\n",
    "change_cut = 5\n",
    "alpha_list = np.round(np.arange(0.01, 1.0, 0.1), 2)        # ì„±ëŠ¥ ì¤‘ì‹¬ êµ¬ê°„\n",
    "threshold_list = np.round(np.arange(0.1, 1.0, 0.1), 2)   # ìœ íš¨ ë²”ìœ„ ì••ì¶•\n",
    "weight_list = np.round(np.arange(6.0, 16.0, 1.0), 2)      # class_weight_1 (í¸ì§‘ë¨) \n",
    "\n",
    "# âœ… ê²°ê³¼ ì €ì¥\n",
    "final_results = []\n",
    "\n",
    "# ğŸ¯ ë°ì´í„° ì¤€ë¹„\n",
    "df = df_base.copy()\n",
    "df['next_day_close'] = df['ë¯¸í™˜ìœ¨_ì¢…ê°€'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['ë¯¸í™˜ìœ¨_ì¢…ê°€']\n",
    "df['target'] = (df['change'] <= -change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', 'ê¸°ì¤€ë…„ì›”', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ğŸ¯ ì‹œí€€ìŠ¤ êµ¬ì„±\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# ğŸ¯ train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# âœ… íŠœë‹ ì‹œì‘\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # ëª¨ë¸ êµ¬ì„±\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(64, return_sequences=True)(inputs)\n",
    "        x = LSTM(32)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "\n",
    "# âœ… ìµœì  ì¡°í•© ì„ íƒ\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# âœ… ìµœì  ì¡°í•© ì¬ì ìš©\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = LSTM(64, return_sequences=True)(inputs)\n",
    "x = LSTM(32)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# âœ… ìµœì¢… ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\nâœ… [ìµœì¢… ì¡°í•© ì ìš© ê²°ê³¼]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\nğŸ“Š ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ğŸ§© í˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea231517-1b74-49dd-8440-28c2e61b9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ê°œì„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc7620b4-b28f-4d47-8de1-1f9a80c97949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "[weight=4.0, threshold=0.45] â†’ macro_f1: 0.4216\n",
      "[weight=4.0, threshold=0.5] â†’ macro_f1: 0.4216\n",
      "[weight=4.0, threshold=0.55] â†’ macro_f1: 0.4216\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "[weight=5.0, threshold=0.45] â†’ macro_f1: 0.4216\n",
      "[weight=5.0, threshold=0.5] â†’ macro_f1: 0.4216\n",
      "[weight=5.0, threshold=0.55] â†’ macro_f1: 0.4216\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "[weight=6.0, threshold=0.45] â†’ macro_f1: 0.4216\n",
      "[weight=6.0, threshold=0.5] â†’ macro_f1: 0.4216\n",
      "[weight=6.0, threshold=0.55] â†’ macro_f1: 0.4216\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\n",
      "âœ… [ìµœì¢… ì¡°í•© ì ìš© ê²°ê³¼]\n",
      "change_cut: 5, alpha: 0.5, threshold: 0.4500, class_weight_1: 4.0, macro f1-score: 0.4216\n",
      "\n",
      "ğŸ“Š ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84       390\n",
      "           1       0.00      0.00      0.00       145\n",
      "\n",
      "    accuracy                           0.73       535\n",
      "   macro avg       0.36      0.50      0.42       535\n",
      "weighted avg       0.53      0.73      0.61       535\n",
      "\n",
      "ğŸ§© í˜¼ë™ í–‰ë ¬:\n",
      "[[390   0]\n",
      " [145   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\campus4D008\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\campus4D008\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\campus4D008\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# âœ… Seed ê³ ì •\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# âœ… Focal Loss ì •ì˜\n",
    "def focal_loss(gamma=2., alpha=0.5):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1)) \\\n",
    "               -tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •\n",
    "change_cut = 5\n",
    "alpha_list = [0.5]\n",
    "threshold_list = [0.3, 0.35, 0.4]\n",
    "weight_list = [6.0, 7.0, 8.0]\n",
    "\n",
    "final_results = []\n",
    "\n",
    "# ğŸ¯ ë°ì´í„° ì¤€ë¹„\n",
    "df = df_base.copy()\n",
    "df['next_day_close'] = df['ë¯¸í™˜ìœ¨_ì¢…ê°€'].shift(-1)\n",
    "df['change'] = df['next_day_close'] - df['ë¯¸í™˜ìœ¨_ì¢…ê°€']\n",
    "df['target'] = (df['change'] >= change_cut).astype(int)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['Date', 'ê¸°ì¤€ë…„ì›”', 'return', 'return_future', 'target', 'next_day_close', 'change'], errors='ignore')\n",
    "y = df['target'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ğŸ¯ ì‹œí€€ìŠ¤ êµ¬ì„±\n",
    "seq_length = 10\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(seq_length, len(X_scaled)):\n",
    "    X_seq.append(X_scaled[i-seq_length:i])\n",
    "    y_seq.append(y[i])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# ğŸ¯ train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# âœ… íŠœë‹ ì‹œì‘\n",
    "for class_weight_1 in weight_list:\n",
    "    class_weights = {0: 1.0, 1: class_weight_1}\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        # ëª¨ë¸ êµ¬ì„±\n",
    "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        x = LSTM(128, return_sequences=True)(inputs)\n",
    "        x = LSTM(64)(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=alpha), metrics=['accuracy'])\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=0,\n",
    "                  class_weight=class_weights)\n",
    "\n",
    "        y_proba = model.predict(X_test).flatten()\n",
    "\n",
    "        for threshold in threshold_list:\n",
    "            y_pred = (y_proba > threshold).astype(int)\n",
    "            macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            final_results.append({\n",
    "                'alpha': alpha,\n",
    "                'threshold': threshold,\n",
    "                'class_weight_1': class_weight_1,\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            print(f\"[weight={class_weight_1}, threshold={threshold}] â†’ macro_f1: {macro_f1:.4f}\")\n",
    "\n",
    "# âœ… ìµœì  ì¡°í•© ì„ íƒ\n",
    "df_results = pd.DataFrame(final_results)\n",
    "best_row = df_results.loc[df_results['macro_f1'].idxmax()]\n",
    "best_alpha = best_row['alpha']\n",
    "best_threshold = best_row['threshold']\n",
    "best_weight = best_row['class_weight_1']\n",
    "\n",
    "# âœ… ìµœì  ì¡°í•© ì¬ì ìš©\n",
    "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = LSTM(128, return_sequences=True)(inputs)\n",
    "x = LSTM(64)(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=best_alpha), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=100,\n",
    "          batch_size=32,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "          verbose=0,\n",
    "          class_weight={0: 1.0, 1: best_weight})\n",
    "\n",
    "# âœ… ìµœì¢… ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\nâœ… [ìµœì¢… ì¡°í•© ì ìš© ê²°ê³¼]\")\n",
    "print(f\"change_cut: {change_cut}, alpha: {best_alpha}, threshold: {best_threshold:.4f}, class_weight_1: {best_weight}, macro f1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "print(\"\\nğŸ“Š ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ğŸ§© í˜¼ë™ í–‰ë ¬:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a871cf6-25d2-48e2-bd9a-01888150fa25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
